{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317a4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret_key import openapi_key\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = openapi_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cc4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211517ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# llm = OpenAI(temperature = .1,\n",
    "#             max_tokens=150,\n",
    "#     top_p=0.85,\n",
    "#     frequency_penalty=0.4,\n",
    "#     presence_penalty=0.3)\n",
    "\n",
    "\n",
    "llm = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68618d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"El Rey del Sabor\" (The King of Flavor)\n"
     ]
    }
   ],
   "source": [
    "name = llm(\"I want to open a restaurant for mexican food. Suggest a royal name for this.\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a780b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3d3c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step. Finish the answer in 25 words\"\"\"\n",
    "\n",
    "prompt_template_name1 = PromptTemplate(template=template, input_variables=[\"answer the question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3db257e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_chain2 = LLMChain(prompt=prompt_template_name1, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7fb9209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nArtificial intelligence (AI) has its roots in ancient Greek mythology, but the modern history of AI began in the 1950s with the development of early computer programs and the term itself being coined. Since then, AI has gone through several phases of excitement and disappointment, with significant breakthroughs happening in the 1980s and 1990s. In recent years, advancements in machine learning and deep learning have led to significant progress in AI, with applications in various fields such as healthcare, finance, and transportation.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 3\n",
    "question = \"Provide a brief overview of the history of artificial intelligence.\"\n",
    "\n",
    "llm_chain2.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abd5a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06c89f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 1\n",
    "prompt_template_name=PromptTemplate(input_variables=[\"cuisine\"],\n",
    "                                    template=\"i want to open a restaurant for this {cuisine}, suggest a fancy name for it\")\n",
    "\n",
    "name_chain=LLMChain(llm=llm, prompt=prompt_template_name,output_key=\"restaurant_name\")\n",
    " \n",
    "# Prompt 2\n",
    "prompt_templates_items=PromptTemplate(input_variables=[\"restaurant_name\"],\n",
    "                                      template=\"suggest 3 fancy and unique menu items for this {restaurant_name}\")\n",
    "food_items_chain=LLMChain(llm=llm,prompt=prompt_templates_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "daec4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=SequentialChain(\n",
    "    chains=[name_chain, food_items_chain], input_variables=[\"cuisine\"],\n",
    "    output_variables=[\"restaurant_name\", \"menu_items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f43539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'mexican',\n",
       " 'restaurant_name': '\\n\\n\"La Casa de México\" ',\n",
       " 'menu_items': '\\n\\n1. \"Chiles en Nogada Relleno de Mariscos\" - A traditional Mexican dish with a twist, featuring poblano peppers stuffed with a delicious seafood mix, topped with a creamy walnut sauce and pomegranate seeds.\\n2. \"Carnitas de Pato Tacos\" - Slow-cooked duck confit served in homemade corn tortillas, topped with a tangy tomatillo salsa and pickled red onions.\\n3. \"Mole de Plátano Relleno de Pollo\" - A creative take on the classic mole sauce, made with ripe plantains and filled with tender chicken, served with a side of cinnamon and chili-spiced rice.'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"cuisine\": \"mexican\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43d7c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Template\n",
    "prompt_template_name = PromptTemplate(input_variables=[\"industry\"],\n",
    "                                    template = \"I want to start a startup for {industry} , suggest me a good name for this\")\n",
    "\n",
    "name_chain = LLMChain(llm = llm, prompt = prompt_template_name)\n",
    "\n",
    "# Second Template\n",
    "prompt_template_items = PromptTemplate( input_variables = [\"name\"],\n",
    "                                     template = \"suggest 3 business strategies in bullet points for {name}\")\n",
    "\n",
    "strategies_chain = LLMChain(llm = llm, prompt = prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e8eb40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cbf9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SimpleSequentialChain(chains=[name_chain, strategies_chain]) # This gives us one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cfc1bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "1. SwiftShip\n",
      "- Offer expedited shipping options for time-sensitive deliveries\n",
      "- Utilize technology to streamline the shipping process and improve efficiency\n",
      "- Develop partnerships with major carriers to expand service offerings and reach new markets\n",
      "\n",
      "2. CargoHive\n",
      "- Offer competitive pricing and flexible shipping options to attract diverse customer base\n",
      "- Focus on sustainability and eco-friendly shipping practices to differentiate from competitors\n",
      "- Implement a customer loyalty program to encourage repeat business and referrals\n",
      "\n",
      "3. CarrierConnect\n",
      "- Offer a digital platform for businesses to connect with and manage multiple carriers\n",
      "- Provide real-time tracking and analytics to improve supply chain visibility for customers\n",
      "- Collaborate with carriers to negotiate competitive rates and improve service offerings \n",
      "\n",
      "4. FreightFlow\n",
      "- Specialize in last-mile delivery services for e-commerce businesses\n",
      "- Utilize data analysis and predictive technology to optimize delivery routes and reduce costs\n",
      "- Offer customizable delivery options such as same-day, next-day, and scheduled deliveries\n",
      "\n",
      "5. MoveMates\n",
      "- Differentiate from traditional logistics companies by offering white-glove moving services for high-end or fragile items\n",
      "- Develop partnerships with interior design and home staging companies to expand clientele\n",
      "- Implement a referral program to incentivize satisfied customers to refer new business.\n",
      "\n",
      "6. LogiNexus\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"logistics\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61682f",
   "metadata": {},
   "source": [
    "## Define a Function to Get Completion__\n",
    "The __get_completion__ function is responsible for sending a prompt to the OpenAI model and receiving its response.\n",
    "\n",
    "__Parameters:__\n",
    "\n",
    "- __prompt__: It is the text input for which the model will generate a completion.\n",
    "- __model__: The gpt-3.5-turbo model is used to perform the tasks.\n",
    "\n",
    "The __openai.ChatCompletion.create__ function is used to send a request to the OpenAI API.\n",
    "\n",
    "This request includes the model, the input messages (formatted as a list of dictionaries with user roles and content), and a temperature setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5bc61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    # Create the messages list with the user prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Create a chat completion request\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "        top_p=0.8\n",
    "\n",
    "    )\n",
    "    \n",
    "    # Return the content of the first response\n",
    "    return response.choices[0].message.content\n",
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac6aea",
   "metadata": {},
   "source": [
    "# Zero Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9fcb1534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Salut, bonjour !\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Translate the following English sentence to French:\n",
    "\n",
    "'Hey, Good Morning!'\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "58675d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Salut, bonjour !\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Translate the following English sentence to French:\n",
    "\n",
    "'Hey, Good Morning!'\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c63545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk was born in South Africa, moved to Canada, studied in the US, founded successful companies like PayPal and SpaceX.\n"
     ]
    }
   ],
   "source": [
    "# Text Summarization\n",
    "\n",
    "prompt = \"\"\"\n",
    "Summarize the following text in 20 words:\n",
    "\n",
    "'Musk was born in Pretoria to model Maye and businessman and engineer Errol Musk, and briefly attended the University of Pretoria before immigrating to Canada at age 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University at Kingston in Canada. Musk later transferred to the University of Pennsylvania and received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University, but dropped out after two days and, with his brother Kimbal, co-founded online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999. That same year, Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal. In October 2002, eBay acquired PayPal for $1.5 billion. Using $100 million of the money he made from the sale of PayPal, Musk founded SpaceX, a spaceflight services company, in 2002.'\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2fca82f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Italy is Rome.\n"
     ]
    }
   ],
   "source": [
    "# Question Answering\n",
    "\n",
    "prompt = \"\"\"\n",
    "Q: What is the capital of Italy?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "47550038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Feedback: \"Your customer support was helpful, Not very satisfied.\"\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e0a45",
   "metadata": {},
   "source": [
    "# One Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "407127b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Delhi\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Q: What is the capital of Italy?\n",
    "A: \"Imperium Romanum in aeternum stabit.\", Rome\n",
    "\n",
    "Q: What is the capital of India?\n",
    "A:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8e9a6",
   "metadata": {},
   "source": [
    "# Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f02e4a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"भारतीय साम्राज्य सदैव टिके रहना चाहिए।\", New Delhi\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "Q: What is the capital of Italy?\n",
    "A: \"Imperium Romanum in aeternum stabit.\", Rome\n",
    "\n",
    "Q: What is the capital of France?\n",
    "A: \"L'Empire romain devrait tenir pour l'éternité.\", Paris\n",
    "\n",
    "Q: What is the capital of Japan?\n",
    "A: \"日本は永遠に生き続けるべきです。\", Tokyo\n",
    "\n",
    "Q: What is the capital of Germany?\n",
    "A: \"Das Deutsche Reich sollte für immer bestehen.\", Berlin\n",
    "\n",
    "Q: What is the capital of Brazil?\n",
    "A: \"O Império Brasileiro deve durar para sempre.\", Brasília\n",
    "\n",
    "Q: What is the capital of India?\n",
    "A:\n",
    "\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e53cc4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like a mixed sentiment, leaning towards negative. We will work on improving our customer support to better satisfy you in the future.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Feedback: \"I loved the quick service and friendly staff.\"\n",
    "Classification: Great we are delighed to have a Positive sentiment\n",
    "\n",
    "Feedback: \"The product did not meet my expectations.\"\n",
    "Classification: Oops, we are afraid its a Negative sentiment\n",
    "\n",
    "Feedback: \"I am not sure if this is the right product for me.\"\n",
    "Classification: We will try to improve and satisfy you next time as its a Neutral sentiment\n",
    "\n",
    "Feedback: \"Your customer support was helpful, Not very satisfied.\"\n",
    "Classification:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7ffd67fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Renew Your Subscription Now!\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Task: Generate a subject line for the following emails.\n",
    "\n",
    "Email: \"Thank you for your purchase! Your order has been shipped and will arrive in 3-5 business days.\"\n",
    "Subject: \"Your Order Has Shipped!\"\n",
    "\n",
    "Email: \"We noticed you left items in your cart. Don’t miss out on these great deals!\"\n",
    "Subject: \"Don’t Forget: Items Waiting in Your Cart!\"\n",
    "\n",
    "Email: \"We’ve just launched a new product line. Check out our latest arrivals and get 10% off your first purchase.\"\n",
    "Subject: \"Explore Our New Arrivals & Enjoy 10% Off!\"\n",
    "\n",
    "Email: \"Your subscription is about to expire. Renew now to continue enjoying our services.\"\n",
    "Subject:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87662d",
   "metadata": {},
   "source": [
    "# Chain Of Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "261d631f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the condition that it is raining, the solution is to carry an umbrella. This will help protect you from getting wet in the rain. Thank you for following the steps and logic provided. Stay dry!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "let's break down the problem step by step\n",
    "\n",
    "1. Chek the condition: is it raining?\n",
    "2. if yes, then aswers should be something like Carry an umbrella\n",
    "3. If it is not raining, the answer hould be something like Do not carry umbrella, but be careful\n",
    "4. provide the final solution to the user exxplining the logic.\n",
    "\n",
    "\n",
    "it is raining.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3c9281cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: [3,1,4,6,5,9,2]\n",
      "Step 2: [1,3,4,6,5,9,2]\n",
      "Step 3: [1,2,4,6,5,9,3]\n",
      "Step 4: [1,2,3,6,5,9,4]\n",
      "Step 5: [1,2,3,4,5,9,6]\n",
      "Step 6: [1,2,3,4,5,6,9]\n",
      "\n",
      "Sorted list: [1,2,3,4,5,6,9]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "let's sort the values of the list step by step\n",
    "\n",
    "1. Start with the unsorted list.\n",
    "2. Compare each elements and find the smallest value.\n",
    "3. Place the samllest value in the first position.\n",
    "4. Repeat the process for all the remaming elements.\n",
    "5. provide the sorted list.\n",
    "\n",
    "Sort the list : [3,1,4,6,5,9,2]\n",
    "\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "64aa6fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Positive aspect: \"The product is well designed\" - Score: 8/10\n",
      "2. Negative aspect: \"less functional\" - Score: 6/10\n",
      "3. Weighting: Positive aspect (8) x 0.6 (weight) = 4.8, Negative aspect (6) x 0.4 (weight) = 2.4\n",
      "4. Final sentiment classification: Overall sentiment is slightly positive. The positive aspect of the well-designed product carries more weight in the overall sentiment analysis, leading to a final sentiment classification of slightly positive.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "let's analyze the sentiment of the review step by step\n",
    "\n",
    "1. Identify the Positive aspect of the review and give a score from 10 \n",
    "2. Identify the Negative aspect of the review and give a score from 10\n",
    "3. weight the positive and negative aspect to determine the overall sentiment.\n",
    "4. provide the final sentiment classification with justification for scores used in above steps\n",
    "\n",
    "Review: \"The product is well designed but less functional\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "35341e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "Sort the values of the list \n",
    "\n",
    "Sort the list : [3,1,4,6,5,9,2]\n",
    "\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b7cc9",
   "metadata": {},
   "source": [
    "# Sequential or Conversational Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "78721b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Imagine you are a detective trying to solve a mystery.\n",
      "Response: As a detective, I am faced with a perplexing mystery that has left the community in fear and confusion. The case involves a series of seemingly random disappearances of young women in the area. The only connection between the victims is that they were all last seen near a local park.\n",
      "\n",
      "I begin my investigation by interviewing witnesses who were in the park around the time of the disappearances. One witness mentions seeing a suspicious-looking man lurking around the park on multiple occasions. I decide to focus my attention on this lead and start looking into the man's background.\n",
      "\n",
      "After conducting a thorough background check, I discover that the man has a history of violence and has been known to frequent the park. I bring him in for questioning, but he denies any involvement in the disappearances. However, during the interrogation, he slips up and reveals details about the victims that were not released to the public.\n",
      "\n",
      "With this new information, I obtain a search warrant for the man's property. Upon searching his home, I find evidence linking him to the disappearances, including personal belongings of the victims and a hidden room in his basement where he kept them captive.\n",
      "\n",
      "I arrest the man and bring him to justice, bringing closure to the families of the victims and restoring peace to the community. Another mystery solved, thanks to my keen investigative skills and determination to seek justice for the victims.\n",
      "\n",
      "Prompt: You arrive at the crime scene and start looking for clues.\n",
      "Response: You notice a broken window with shards of glass scattered on the ground. There are also muddy footprints leading away from the window. As you continue to search the area, you find a discarded glove with a small bloodstain on it. \n",
      "\n",
      "You take note of these clues and begin to piece together what may have happened at the crime scene. The broken window suggests a forced entry, and the muddy footprints indicate that the perpetrator may have fled on foot. The bloodstained glove could potentially belong to the suspect, providing a DNA sample for further analysis.\n",
      "\n",
      "You also notice a nearby security camera and make a mental note to review the footage later to see if it captured any suspicious activity. As you continue to search the area, you find a piece of torn fabric caught on a nearby fence, which could potentially be linked to the suspect's clothing.\n",
      "\n",
      "After collecting these clues, you document them carefully and call for backup to help process the scene. With the evidence in hand, you begin to piece together the timeline of events and work towards identifying the perpetrator responsible for the crime.\n",
      "\n",
      "Prompt: You find a strange object at the crime scene. What is it?\n",
      "Response: It appears to be a small, intricately carved wooden figurine of a dragon. The figurine is covered in strange symbols and markings, and its eyes seem to glow faintly in the dim light of the crime scene. It is unclear what significance this object may have in relation to the crime, but its presence is certainly unusual and mysterious.\n",
      "\n",
      "Prompt: How does this object relate to the crime?\n",
      "Response: Without knowing what the object is or any details about the crime, it is impossible to determine how they may be related. More information would be needed to make a connection between the object and the crime.\n",
      "\n",
      "Prompt: Who do you think is the suspect and why?\n",
      "Response: Without more information about the situation or context, it is impossible to determine who the suspect may be. It is important to gather evidence and information before making any accusations or assumptions about someone's guilt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Imagine you are a detective trying to solve a mystery.\",\n",
    "    \"You arrive at the crime scene and start looking for clues.\",\n",
    "    \"You find a strange object at the crime scene. What is it?\",\n",
    "    \"How does this object relate to the crime?\",\n",
    "    \"Who do you think is the suspect and why?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = get_completion(prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e034b3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Describe the issue you are experiencing with your device.\n",
      "Response: I am experiencing slow performance and frequent freezing on my device. It takes a long time to open apps and switch between them, and sometimes the device becomes unresponsive and I have to restart it.\n",
      "\n",
      "Prompt: Have you tried restarting your device? If not, please do so and describe what happens.\n",
      "Response: I'm sorry, I am an AI and do not have the ability to restart a physical device. However, if you are experiencing technical issues with your device, I recommend restarting it to see if that resolves the problem. If you continue to experience issues, please provide more details so I can try to assist you further.\n",
      "\n",
      "Prompt: What is the model number of your device?\n",
      "Response: I'm sorry, I am a virtual assistant and do not have a physical device. I am here to assist you with any questions or information you may need.\n",
      "\n",
      "Prompt: Based on the model number, these are common issues. Which one matches your problem?\n",
      "Response: I'm sorry, but you did not provide a model number or list any common issues. Please provide more information so I can assist you further.\n",
      "\n",
      "Prompt: Here are the steps to resolve this issue. Follow them one by one, and let me know if the problem persists.\n",
      "Response: 1. Restart your device: Sometimes a simple restart can resolve many technical issues. Turn off your device, wait a few seconds, and then turn it back on.\n",
      "\n",
      "2. Check your internet connection: Make sure you have a stable internet connection. If you are using Wi-Fi, try restarting your router or connecting to a different network.\n",
      "\n",
      "3. Clear cache and cookies: Clearing your browser's cache and cookies can help resolve loading issues. Go to your browser settings and clear the cache and cookies.\n",
      "\n",
      "4. Update your browser: Make sure you are using the latest version of your browser. Outdated browsers can sometimes cause loading problems.\n",
      "\n",
      "5. Disable browser extensions: Some browser extensions can interfere with website loading. Try disabling any extensions you have installed and see if the issue persists.\n",
      "\n",
      "6. Try a different browser: If the problem continues, try accessing the website using a different browser to see if the issue is browser-specific.\n",
      "\n",
      "7. Contact the website's support: If none of the above steps resolve the issue, reach out to the website's support team for further assistance. They may be able to provide specific troubleshooting steps for their site.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Describe the issue you are experiencing with your device.\",\n",
    "    \"Have you tried restarting your device? If not, please do so and describe what happens.\",\n",
    "    \"What is the model number of your device?\",\n",
    "    \"Based on the model number, these are common issues. Which one matches your problem?\",\n",
    "    \"Here are the steps to resolve this issue. Follow them one by one, and let me know if the problem persists.\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = get_completion(prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a7339457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Please describe the main symptoms you are experiencing.\n",
      "Response: I'm sorry, but as an AI, I do not have any physical sensations or symptoms. I am here to assist you with any questions or concerns you may have. If you are experiencing symptoms, it is important to consult with a healthcare professional for an accurate diagnosis and appropriate treatment.\n",
      "\n",
      "Prompt: How long have you been experiencing these symptoms?\n",
      "Response: I'm sorry, I am not experiencing any symptoms as I am an AI assistant. How can I assist you today?\n",
      "\n",
      "Prompt: Do you have any pre-existing conditions or allergies?\n",
      "Response: As an AI, I do not have any physical health conditions or allergies.\n",
      "\n",
      "Prompt: Have you traveled recently or come into contact with anyone who is sick?\n",
      "Response: I am an AI and do not have the ability to travel or come into contact with anyone who is sick.\n",
      "\n",
      "Prompt: Based on your symptoms and history, these are potential diagnoses. Which one seems most accurate to you?\n",
      "Response: I'm sorry, but I am not able to provide medical diagnoses as I am not a healthcare professional. It is important to consult with a doctor or healthcare provider for an accurate diagnosis and appropriate treatment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Please describe the main symptoms you are experiencing.\",\n",
    "    \"How long have you been experiencing these symptoms?\",\n",
    "    \"Do you have any pre-existing conditions or allergies?\",\n",
    "    \"Have you traveled recently or come into contact with anyone who is sick?\",\n",
    "    \"Based on your symptoms and history, these are potential diagnoses. Which one seems most accurate to you?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = get_completion(prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e46feac",
   "metadata": {},
   "source": [
    "# Self-Consistency Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "599e2c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 30-pound weight is heavier than 1000 feathers.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Let's consider which is heavier: 1000 feathers or a 30-pound weight. I'll think through this in a few different ways and then decide which answer seems most consistent.\n",
    "\n",
    "1. First line of reasoning: A single feather is very light, almost weightless. So, 1000 feathers might still be quite light, possibly lighter than a 30-pound weight.\n",
    "\n",
    "2. Second line of reasoning: 1000 is a large number, and when you add up the weight of so many feathers, it could be quite heavy. Maybe it's heavier than a 30-pound weight.\n",
    "\n",
    "3. Third line of reasoning: The average weight of a feather is very small. Even 1000 feathers would not add up to 30 pounds.\n",
    "\n",
    "Considering these reasonings, the most consistent answer is:\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b6145323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. First approach:\n",
      "15 - 3 = 12\n",
      "12 + 8 = 20\n",
      "20 - 5 = 15\n",
      "\n",
      "2. Second approach:\n",
      "8 - 5 = 3\n",
      "15 - 3 + 3 = 15\n",
      "\n",
      "3. Third approach:\n",
      "Starting with 15 apples:\n",
      "15 - 3 = 12\n",
      "12 + 8 = 20\n",
      "20 - 5 = 15\n",
      "\n",
      "In all three approaches, we arrived at the same answer of 15 apples. This confirms that the solution is consistent regardless of the method used.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I will solve the following math problem in several different ways and check if I arrive at the same answer each time.\n",
    "\n",
    "Problem: If you have 15 apples and you give away 3, then gain 8 more, and finally lose 5, how many apples do you have?\n",
    "\n",
    "1. First approach: Start by subtracting 3 from 15, then add 8, and subtract 5.\n",
    "\n",
    "2. Second approach: Combine the gains and losses first (8 - 5), then apply the net change to the original number of apples (15 - 3 + net change).\n",
    "\n",
    "3. Third approach: Apply each operation in sequence and keep track of the apples after each step.\n",
    "\n",
    "Let's see which approach gives the most consistent result.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7af76",
   "metadata": {},
   "source": [
    "# Tree-Of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "73fd92e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:\n",
      "The dimensions should be 40 meters by 20 meters, creating a rectangular field with an area of 800 square meters.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Solve the problem: A farmer has 100 meters of fencing and wants to enclose the maximum area for his rectangular field. What should the dimensions be?\n",
    "\n",
    "Let's think about this in a few ways:\n",
    "\n",
    "1. If the field is a square, each side would be 100 / 4 = 25 meters. The area would be 25 * 25 = 625 square meters.\n",
    "\n",
    "2. What if the field is not a square? Let's try a 2:1 ratio. The lengths would be 40 and 20 meters. The area would be 40 * 20 = 800 square meters.\n",
    "\n",
    "3. Are there any other ratios that might give a larger area than a square or a 2:1 rectangle?\n",
    "\n",
    "Considering these options, the best dimensions for the maximum area are:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"AI Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e4e39fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Fourth branch: Start by visiting city D first, then C, then B, and finally A. Compare this route with the previous ones to determine if it offers any advantages in terms of distance or time.\n",
      "\n",
      "5. Fifth branch: Start by visiting city A first, then C, then B, and finally D. Evaluate if this route provides a more efficient option compared to the other branches.\n",
      "\n",
      "By exploring these different branches and variations, I will be able to determine the most efficient route to visit all four cities. I will consider factors such as distance, time, traffic conditions, and any other relevant variables to make an informed decision.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I need to plan the most efficient route to visit the following cities: A, B, C, and D. I'll explore different routes and their implications, step by step, to find the best option.\n",
    "\n",
    "1. First branch: Start by visiting city A, then move to city B, then to city C, and finally to city D. Consider the total distance and time required.\n",
    "\n",
    "2. Second branch: Start by visiting city B first, then A, then D, and finally C. Evaluate if this route is shorter or faster than the first.\n",
    "\n",
    "3. Third branch: Start by visiting city C first, then D, then A, and finally B. Analyze if this route offers any advantages over the others.\n",
    "\n",
    "At each step, I'll compare the results of each branch and see which provides the most efficient route. If needed, I will explore sub-branches by adjusting the sequence slightly.\n",
    "\n",
    "Let's see which path is the most optimal.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "a352c4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering that August is monsoon season in Goa, it might be best to plan indoor activities or activities that can be enjoyed in the rain. Here are some suggestions:\n",
      "\n",
      "1. Visit local museums or art galleries to learn about Goan culture and history.\n",
      "2. Explore the spice plantations in the area and take a guided tour.\n",
      "3. Attend a cooking class to learn how to make traditional Goan dishes.\n",
      "4. Visit the Dudhsagar Waterfalls, which are at their most impressive during the monsoon season.\n",
      "5. Take a river cruise on the Mandovi River and enjoy the lush greenery along the banks.\n",
      "6. Relax and unwind at a spa or wellness retreat to pamper yourself.\n",
      "\n",
      "Remember to pack appropriate rain gear and clothing for your trip to Goa in August. Enjoy your weekend getaway!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Let's plan a weekend trip by considering multiple options.\n",
    "\n",
    "1. Option 1: Go to the mountains.\n",
    "    - Branch A: If the weather is good in the mountains.\n",
    "        - Sub-branch A1: You can go hiking.\n",
    "        - Sub-branch A2: You can visit a nearby lake.\n",
    "    - Branch B: If the weather is bad in the mountains.\n",
    "        - Sub-branch B1: You will stay in a cabin and relax.\n",
    "        - Sub-branch B2: You can explore local museums.\n",
    "\n",
    "2. Option 2: Go to the beach.\n",
    "    - Branch C: If the weather is sunny at the beach.\n",
    "        - Sub-branch C1: You can swim in the ocean.\n",
    "        - Sub-branch C2: You can sunbathe and play beach volleyball.\n",
    "    - Branch D: If the weather is cloudy or rainy at the beach.\n",
    "        - Sub-branch D1: You will visit indoor attractions like an aquarium.\n",
    "        - Sub-branch D2: You can go shopping in beachside stores.\n",
    "\n",
    "3. Considering all these factors, decide the best option for your weekend trip.\n",
    "\n",
    "What should i do for my trip to Goa in month of august\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ec5a772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the facts of the current case, it seems that there is clear evidence of negligence on the part of the defendant. The plaintiff has provided sufficient evidence to support their claim, and it is likely that the court will rule in favor of the plaintiff and award damages. This outcome aligns with Precedent 1, where a similar case resulted in the plaintiff winning due to the defendant's negligence. However, it is important to consider all possible outcomes and be prepared for any potential legal arguments that the defendant may present.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"\n",
    "Let's analyze the legal case by considering multiple precedents and possible outcomes.\n",
    "\n",
    "1. Precedent 1: A similar case where the plaintiff won.\n",
    "    - Branch A: The court found that the defendant was negligent.\n",
    "        - Sub-branch A1: The plaintiff was awarded damages due to clear evidence of negligence.\n",
    "        - Sub-branch A2: The court ruled in favor of the plaintiff due to the defendant's breach of duty.\n",
    "\n",
    "2. Precedent 2: A similar case where the defendant won.\n",
    "    - Branch B: The court found no negligence on the defendant's part.\n",
    "        - Sub-branch B1: The plaintiff failed to provide sufficient evidence.\n",
    "        - Sub-branch B2: The court ruled that the plaintiff assumed the risk.\n",
    "\n",
    "3. Precedent 3: A case with a mixed outcome.\n",
    "    - Branch C: The court found both parties partially at fault.\n",
    "        - Sub-branch C1: Damages were reduced based on the plaintiff's contributory negligence.\n",
    "        - Sub-branch C2: The court ruled that both parties shared liability, resulting in a split decision.\n",
    "\n",
    "4. Based on the facts of the current case, consider the most likely outcome.\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a06fe6",
   "metadata": {},
   "source": [
    "# Using Jinja2 Template Format__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables - to incluse any valiable we will need {{Var-Name}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "07d333b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "e586ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jinja2_template = \"Give me an {{ adjective }} fact about {{ topic }}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "55997278",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(jinja2_template, template_format=\"jinja2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "a97f216f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me an interesting fact about space exploration\n"
     ]
    }
   ],
   "source": [
    "question = prompt.format(adjective=\"interesting\", topic=\"space exploration\")\n",
    "\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5171bd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:\n",
      "The farthest spacecraft from Earth is Voyager 1, which was launched in 1977 and is currently over 14 billion miles away from Earth. It is the only human-made object to have entered interstellar space, beyond our solar system.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(question)\n",
    "print(\"AI Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "79197727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:\n",
      "Investing in renewable energy is crucial for governments in the next decade for several reasons. \n",
      "\n",
      "First and foremost, renewable energy sources such as solar, wind, and hydro power are sustainable and environmentally friendly alternatives to fossil fuels. As the effects of climate change become increasingly severe, it is imperative that governments take action to reduce greenhouse gas emissions and transition to cleaner energy sources. Investing in renewable energy will help to mitigate the impacts of climate change and protect the planet for future generations.\n",
      "\n",
      "Additionally, investing in renewable energy can help to create jobs and stimulate economic growth. The renewable energy sector is one of the fastest growing industries in the world, and governments that invest in renewable energy will be able to capitalize on this growth and create new opportunities for their citizens. By supporting the development of renewable energy technologies, governments can also attract investment and drive innovation in their economies.\n",
      "\n",
      "Furthermore, investing in renewable energy can help to improve energy security and reduce dependence on imported fossil fuels. By diversifying their energy sources and investing in domestic renewable energy projects, governments can reduce their vulnerability to fluctuations in global energy markets and ensure a stable and reliable energy supply for their citizens.\n",
      "\n",
      "In conclusion, investing in renewable energy is not only important for addressing the urgent challenges of climate change, but also for promoting economic growth, creating jobs, and enhancing energy security. Governments that prioritize renewable energy in the next decade will be better positioned to meet the needs of their citizens and build a more sustainable and prosperous future for all.\n"
     ]
    }
   ],
   "source": [
    "jinja2_template = \"Make a convincing argument for why {{ action }} is important for {{ group }} in {{ time_period }}.\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(jinja2_template, template_format=\"jinja2\")\n",
    "\n",
    "# Format the prompt with specific values for 'action', 'group', and 'time_period'\n",
    "argument_prompt = prompt.format(action=\"investing in renewable energy\", group=\"governments\", time_period=\"the next decade\")\n",
    "\n",
    "response = get_completion(argument_prompt)\n",
    "print(\"AI Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "06972c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:\n",
      "Investing in renewable energy is crucial for governments in the next decade to combat climate change, reduce dependence on fossil fuels, create jobs in the growing green economy, and ensure energy security. Transitioning to clean energy sources will also lead to improved public health and a more sustainable future for all.\n"
     ]
    }
   ],
   "source": [
    "jinja2_template = \"In 50 words or less, Make a convincing argument for why {{ action }} is important for {{ group }} in {{ time_period }}.\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(jinja2_template, template_format=\"jinja2\")\n",
    "\n",
    "# Format the prompt with specific values for 'action', 'group', and 'time_period'\n",
    "argument_prompt = prompt.format(action=\"investing in renewable energy\", group=\"governments\", time_period=\"the next decade\")\n",
    "\n",
    "response = get_completion(argument_prompt)\n",
    "print(\"AI Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9bc935",
   "metadata": {},
   "source": [
    "# String Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "daf40875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "0bd08b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \" Please write a {length} review,of the book {book_title}. Focus on being very precise. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "3d781a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variables = {\n",
    "    \n",
    "    \"length\" : \"Short\",\n",
    "    \"book_title\" : \"House of dragon\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "c05896db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the prompt template\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=input_variables,\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "bfb46fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the prompt by filling in the placeholders with specific values\n",
    "\n",
    "formatted_prompt = prompt.format(**input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "a89c56b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Please write a Short review,of the book House of dragon. Focus on being very precise. '"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "05b0aa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:\n",
      "House of Dragon is a thrilling and captivating read that delves into the world of dragons and power struggles within a royal family. The intricate plot and well-developed characters keep readers engaged from start to finish. The book is a must-read for fans of fantasy and adventure genres.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(formatted_prompt)\n",
    "print(\"AI Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "073dc176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv, yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"file_name.json\",\"r\") as file:\n",
    "    input_Variable = json.load(file)\n",
    "    \n",
    "print(input_Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "09382b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short story about a brave knight who discovers a magic sword in enchanted forest.\n"
     ]
    }
   ],
   "source": [
    "# Define a simple string template with placeholders\n",
    "template_str = \"Write a short story about a {character} who discovers a {object} in {location}.\"\n",
    "\n",
    "# Create a PromptTemplate object with the template string and a list of input variables\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"character\", \"object\", \"location\"],\n",
    "    template=template_str\n",
    ")\n",
    "\n",
    "# Format the prompt by filling in the placeholders with specific values\n",
    "formatted_prompt = prompt.format(character=\"brave knight\", object=\"magic sword\", location=\"enchanted forest\")\n",
    "\n",
    "# Print the formatted prompt\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "aedcb669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:\n",
      "Sir William was known throughout the kingdom as the bravest and most skilled knight in all the land. He had faced countless foes and emerged victorious every time. But one day, while on a quest to rid the kingdom of a fearsome dragon, Sir William found himself lost in the depths of an enchanted forest.\n",
      "\n",
      "As he wandered through the dense foliage, he heard whispers in the wind and felt a strange energy pulsing through the air. Intrigued, he followed the source of the magic until he stumbled upon a clearing where a magnificent sword lay embedded in a stone.\n",
      "\n",
      "Without hesitation, Sir William approached the sword and grasped its hilt. As soon as his fingers touched the cold metal, a surge of power coursed through his veins. The sword glowed with a brilliant light, illuminating the entire forest.\n",
      "\n",
      "Realizing that he had found a truly magical weapon, Sir William knew that he had been chosen for a great destiny. With the sword in hand, he set out to find the dragon and fulfill his quest.\n",
      "\n",
      "As he neared the dragon's lair, Sir William could feel the beast's fiery breath and hear its menacing roar. But he did not falter. With the magic sword in hand, he charged into battle with a newfound confidence and determination.\n",
      "\n",
      "The dragon lunged at him with its sharp claws and fiery breath, but Sir William deftly dodged its attacks and struck back with the sword. With each blow, the dragon weakened until finally, with one final strike, Sir William plunged the sword into the beast's heart, vanquishing it once and for all.\n",
      "\n",
      "The kingdom rejoiced at the news of the dragon's defeat, and Sir William was hailed as a hero. But he knew that it was not his skill or bravery alone that had won the day. It was the magic sword that had guided his hand and granted him the strength to overcome any obstacle.\n",
      "\n",
      "And so, Sir William continued on his adventures, wielding the magic sword with pride and honor, knowing that he was destined for greatness.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(formatted_prompt)\n",
    "print(\"AI Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7511876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write a short review of the book 'Dune' by Frank Herbert. Focus on the character development.\n"
     ]
    }
   ],
   "source": [
    "template = \"Please write a {length} review of the book '{book_title}' by {author}. Focus on the {aspect}.\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"length\", \"book_title\", \"author\", \"aspect\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    length=\"short\",\n",
    "    book_title=\"Dune\",\n",
    "    author=\"Frank Herbert\",\n",
    "    aspect=\"character development\"\n",
    ")\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8779b1",
   "metadata": {},
   "source": [
    "# String Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e869d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "f6681ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template(\"Hello, my name is $name and i am $age years old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "b95af57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_template = template.substitute(name = \"Raghav\", age = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "8f586163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my name is Raghav and i am 50 years old'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2113915",
   "metadata": {},
   "source": [
    "# String Format \"str.format\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2e2cbb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Hello, my name is {name} and i am {age} years old\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "d067a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t = template.format(name = \"Raghav\", age = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "9fbd42f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my name is Raghav and i am 50 years old'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ee640",
   "metadata": {},
   "source": [
    "# Using 'f - String'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "d90eceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Raghav\"\n",
    "age = 50\n",
    "\n",
    "f_mm = f'Hello, my name is {name} and i am {age} years old.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "6cfbf90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my name is Raghav and i am 50 years old.'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40523a87",
   "metadata": {},
   "source": [
    "# Creating a Custom Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05503051",
   "metadata": {},
   "source": [
    "### Pydantic is a python library which is used very extensively for data validation and setting management.\n",
    "### Used to define the data models with annotations and it gives automatic validation and parsing of the incoming data based on the annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e4444",
   "metadata": {},
   "source": [
    "## Pydantic is highly used in applications where we need to ensure that data conforms to certain type  and constraints.\n",
    "\n",
    "### Examples - Web Development (Validating API Request Data) and Configuration management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57996d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\\\n",
    "Given the book title, generate a brief summary of the book.\n",
    "Book Title: {book_title}\n",
    "Summary:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4b950307",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (452789413.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [167]\u001b[0;36m\u001b[0m\n\u001b[0;31m    invalid_user = User(**invalid_data)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import List\n",
    "\n",
    "# Define a Pydantic model\n",
    "class User(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    email: str\n",
    "    age: int\n",
    "    friends: List[int] = []\n",
    "\n",
    "# # Valid data\n",
    "# data = {\n",
    "#     \"id\": 123,\n",
    "#     \"name\": \"Alice\",\n",
    "#     \"email\": \"alice@example.com\",\n",
    "#     \"age\": 25,\n",
    "#     \"friends\": [234, 345, 456]\n",
    "# }\n",
    "\n",
    "# # Instantiate the model with valid data\n",
    "# user = User(**data)\n",
    "# print(user)\n",
    "\n",
    "# Invalid data (age should be an integer)\n",
    "invalid_data = {\n",
    "    \"id\": 124,\n",
    "    \"name\": \"Bob\",\n",
    "    \"email\": \"bob@example.com\",\n",
    "    \"age\": \"twenty-five\",  # Invalid age\n",
    "    \"friends\": [234, 345]\n",
    "}\n",
    "\n",
    "user = User(**invalid_data)\n",
    "print(user)\n",
    "# try:\n",
    "#     invalid_user = User(**invalid_data)\n",
    "# except ValidationError as e:\n",
    "#     print(e.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d9fb6d93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for User\nage\n  Must be above 18 (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [173]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m123\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfriends\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m234\u001b[39m, \u001b[38;5;241m345\u001b[39m, \u001b[38;5;241m456\u001b[39m]\n\u001b[1;32m     25\u001b[0m }\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Instantiate the model with valid data\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m user \u001b[38;5;241m=\u001b[39m \u001b[43mUser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(user)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for User\nage\n  Must be above 18 (type=value_error)"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, ValidationError, validator\n",
    "from typing import List\n",
    "\n",
    "# Define a Pydantic model\n",
    "class User(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    email: str\n",
    "    age: int\n",
    "    friends: List[int] = []\n",
    "        \n",
    "    @validator(\"age\")\n",
    "    def check_age(cls, value):\n",
    "        if value < 18:\n",
    "            raise ValueError(\"Must be above 18\")\n",
    "        return value\n",
    "\n",
    "# Valid data\n",
    "data = {\n",
    "    \"id\": 123,\n",
    "    \"name\": \"Alice\",\n",
    "    \"email\": \"alice@example.com\",\n",
    "    \"age\": 15,\n",
    "    \"friends\": [234, 345, 456]\n",
    "}\n",
    "\n",
    "# Instantiate the model with valid data\n",
    "user = User(**data)\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "abc3ec60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the book title, generate a brief summary of the book.\n",
      "Book Title: The Great Gatsby\n",
      "Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class StringPromptTemplate:\n",
    "    def __init__(self, template):\n",
    "        self.template = template\n",
    "\n",
    "    def format(self, **kwargs):\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "class BookSummarizerPrompt(BaseModel):\n",
    "    book_title: str\n",
    "    template: StringPromptTemplate\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def create_prompt(self):\n",
    "        return self.template.format(book_title=self.book_title)\n",
    "\n",
    "# Assuming an instance of StringPromptTemplate is created elsewhere\n",
    "template = StringPromptTemplate(\n",
    "    \"\"\"\n",
    "Given the book title, generate a brief summary of the book.\n",
    "Book Title: {book_title}\n",
    "Summary:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Usage\n",
    "book_summarizer_prompt = BookSummarizerPrompt(book_title = \"The Great Gatsby\", template = template)\n",
    "print(book_summarizer_prompt.create_prompt())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "83fb64d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:\n",
      "\"The Great Gatsby\" follows the story of Jay Gatsby, a wealthy and mysterious man who throws extravagant parties in the hopes of rekindling a romance with his former love, Daisy Buchanan. Set in the 1920s, the novel explores themes of love, wealth, and the American Dream, as Gatsby's obsession with Daisy ultimately leads to tragedy. Through the eyes of narrator Nick Carraway, readers are taken on a journey through the glamorous and tumultuous world of the Jazz Age.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(book_summarizer_prompt.create_prompt())\n",
    "print(\"AI Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe642056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995619e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7c6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f007a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f58956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea0245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecdd9eae",
   "metadata": {},
   "source": [
    "# Few Shot Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8889b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: 9, 13, 17, 21, 25, 29\n",
      "Sum: 114\n",
      "Answer: Even\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Task:\n",
    "You're provided with sequences of odd numbers. Your task is to determine if the sum of the numbers in each sequence results in an even number. If the sum is even, respond with \"Even.\" If the sum is odd, respond with \"Odd.\"\n",
    "\n",
    "Examples:\n",
    "\n",
    "Sequence: 3, 7, 15, 21, 8, 11, 4\n",
    "Sum: 57\n",
    "Answer: Odd\n",
    "\n",
    "Sequence: 5, 12, 19, 25, 10, 13, 3\n",
    "Sum: 65\n",
    "Answer: Odd\n",
    "\n",
    "Sequence: 6, 14, 21, 9, 11, 17, 7\n",
    "Sum: 65\n",
    "Answer: Odd\n",
    "\n",
    "Sequence: 8, 16, 24, 10, 13, 18, 5\n",
    "Sum: 26\n",
    "Answer: Even\n",
    "\n",
    "Sequence: 15, 32, 5, 13, 27, 7, 1\n",
    "Sum: 68\n",
    "Answer: Even\n",
    "\n",
    "New Sequence:\n",
    "Sequence: 9, 13, 17, 21, 25, 29\n",
    "Sum:\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86382",
   "metadata": {},
   "source": [
    "# Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f816f128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the best way to learn a new language?\n",
      "The Learning is strong in this one.\n",
      "Summarize our conversation so far in 10 words.\n",
      "Best way to learn new language is through consistent practice.\n",
      "\n",
      "--- Updated Conversation ---\n",
      "\n",
      "What's the best way to learn a new language?\n",
      "The Learning is strong in this one.\n",
      "Summarize our conversation so far in 10 words.\n",
      "Best way to learn new language is through consistent practice.\n",
      "Can you give me some specific strategies?\n",
      "Immersion, practice with native speakers, use language learning apps, study regularly.\n",
      "\n",
      "--- Conversation with Summary ---\n",
      "\n",
      "What's the best way to learn a new language?\n",
      "The Learning is strong in this one.\n",
      "Summarize our conversation so far in 10 words.\n",
      "Best way to learn new language is through consistent practice.\n",
      "Can you give me some specific strategies?\n",
      "Immersion, practice with native speakers, use language learning apps, study regularly.\n",
      "Summarize our conversation so far in exactly 10 words.\n",
      "Consistent practice, immersion, native speakers, language apps, study regularly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    # Create the messages list with the user prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Create a chat completion request\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.4,\n",
    "        top_p=0.8\n",
    "        # Degree of randomness in the model's output\n",
    "    )\n",
    "    \n",
    "    # Return the content of the first response\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Step 1: Import the Necessary Components\n",
    "from langchain.prompts import MessagesPlaceholder, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Step 2: Define Message Templates\n",
    "simple_prompt = \"The {subject} is strong in this one.\"\n",
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "\n",
    "# Step 3: Create a Chat Prompt with Placeholders\n",
    "simple_message_template = HumanMessagePromptTemplate.from_template(simple_prompt)\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"conversation\"),\n",
    "    simple_message_template,\n",
    "    human_message_template\n",
    "])\n",
    "\n",
    "# Step 4: Define the Initial Conversation Messages\n",
    "# Starting the conversation with a user input only, no predefined AI response\n",
    "initial_human_message = HumanMessage(content=\"What's the best way to learn a new language?\")\n",
    "\n",
    "# Step 5: Generate the Initial Conversation\n",
    "initial_conversation = chat_prompt.format_prompt(\n",
    "    conversation=[initial_human_message],\n",
    "    subject=\"Learning\",\n",
    "    word_count=\"10\"\n",
    ").to_messages()\n",
    "\n",
    "# Step 6: Format the Conversation for the LLM\n",
    "conversation_prompt = \"\\n\".join([message.content for message in initial_conversation])\n",
    "\n",
    "# Step 7: Use the OpenAI API to Generate the First AI Response\n",
    "llm_response = get_completion(conversation_prompt, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Step 8: Add the LLM's Response to the Conversation\n",
    "first_ai_message = AIMessage(content=llm_response)\n",
    "initial_conversation.append(first_ai_message)\n",
    "\n",
    "# Step 9: Print the Conversation\n",
    "for message in initial_conversation:\n",
    "    print(message.content)\n",
    "\n",
    "# Step 10: Continue the Conversation Dynamically\n",
    "# Example of continuing the conversation with a new user input\n",
    "# Assuming the same setup as before\n",
    "\n",
    "# Step 10: Continue the Conversation Dynamically\n",
    "# Example of continuing the conversation with a new user input\n",
    "new_human_message = HumanMessage(content=\"Can you give me some specific strategies?\")\n",
    "initial_conversation.append(new_human_message)\n",
    "\n",
    "# Update the prompt with the latest conversation\n",
    "conversation_prompt = \"\\n\".join([message.content for message in initial_conversation])\n",
    "\n",
    "# Get the next AI response\n",
    "new_llm_response = get_completion(conversation_prompt, model=\"gpt-3.5-turbo\")\n",
    "new_ai_message = AIMessage(content=new_llm_response)\n",
    "initial_conversation.append(new_ai_message)\n",
    "\n",
    "# Print the updated conversation\n",
    "print(\"\\n--- Updated Conversation ---\\n\")\n",
    "for message in initial_conversation:\n",
    "    print(message.content)\n",
    "\n",
    "# Requesting a 10-word summary as a separate, explicit instruction\n",
    "summary_request = \"Summarize our conversation so far in exactly 10 words.\"\n",
    "\n",
    "# Add the summary request to the conversation\n",
    "initial_conversation.append(HumanMessage(content=summary_request))\n",
    "\n",
    "# Update the prompt for summary\n",
    "summary_prompt = \"\\n\".join([message.content for message in initial_conversation])\n",
    "\n",
    "# Get the summary response\n",
    "summary_response = get_completion(summary_prompt, model=\"gpt-3.5-turbo\")\n",
    "summary_ai_message = AIMessage(content=summary_response)\n",
    "initial_conversation.append(summary_ai_message)\n",
    "\n",
    "# Print the conversation including the summary\n",
    "print(\"\\n--- Conversation with Summary ---\\n\")\n",
    "for message in initial_conversation:\n",
    "    print(message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "26cf2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b7fc23",
   "metadata": {},
   "source": [
    "# Without Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a3e88b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template_string = \"\"\"Extract the following information from the book review that is delimited by triple backticks:\n",
    "- Book title\n",
    "- Author name\n",
    "- Genre\n",
    "- User rating out of 5\n",
    "\n",
    "Review: ```{text}```\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "book_review = \"\"\"\n",
    "I just finished reading 'The Silent Patient' by Alex Michaelides, and it was an incredible psychological thriller. The way the author builds suspense is masterful, and I couldn't put it down. I'd give it a solid 4.5 out of 5 stars. Definitely worth a read if you're into thrillers!\n",
    "\"\"\"\n",
    "\n",
    "messages = prompt_template.format_messages(text=book_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1e550c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Book title: The Silent Patient\n",
      "- Author name: Alex Michaelides\n",
      "- Genre: Psychological thriller\n",
      "- User rating out of 5: 4.5 out of 5 stars\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4a23f5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c94e284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_parse_response(response):\n",
    "    # Check if response content is available\n",
    "    if not response or not response.content:\n",
    "        return {\"error\": \"No content in the response\"}\n",
    "\n",
    "    lines = response.content.splitlines()\n",
    "    parsed_data = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if \"Book title\" in line:\n",
    "            parsed_data[\"title\"] = line.split(\":\")[1].strip()\n",
    "        elif \"Author name\" in line:\n",
    "            parsed_data[\"author\"] = line.split(\":\")[1].strip()\n",
    "        elif \"Genre\" in line:\n",
    "            parsed_data[\"genre\"] = line.split(\":\")[1].strip()\n",
    "        elif \"User rating out of 5\" in line:\n",
    "            parsed_data[\"rating\"] = float(line.split(\":\")[1].strip())\n",
    "    \n",
    "    return parsed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e1cd5509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The Silent Patient', 'author': 'Alex Michaelides', 'genre': 'Psychological thriller', 'rating': 4.5}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now manually parse the response\n",
    "output_dict = manual_parse_response(response)\n",
    "\n",
    "if \"error\" in output_dict:\n",
    "    print(output_dict[\"error\"])\n",
    "else:\n",
    "    print(output_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134abf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5f96e89",
   "metadata": {},
   "source": [
    "# With Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3c5997d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d89cdb20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"title\": string  // The title of the book.\n",
      "\t\"author\": string  // The author of the book.\n",
      "\t\"genre\": string  // The genre of the book.\n",
      "\t\"rating\": string  // The user's rating of the book out of 5.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# Define the schema for each expected piece of data\n",
    "title_schema = ResponseSchema(\n",
    "    name=\"title\",\n",
    "    description=\"The title of the book.\"\n",
    ")\n",
    "\n",
    "author_schema = ResponseSchema(\n",
    "    name=\"author\",\n",
    "    description=\"The author of the book.\"\n",
    ")\n",
    "\n",
    "genre_schema = ResponseSchema(\n",
    "    name=\"genre\",\n",
    "    description=\"The genre of the book.\"\n",
    ")\n",
    "\n",
    "rating_schema = ResponseSchema(\n",
    "    name=\"rating\",\n",
    "    description=\"The user's rating of the book out of 5.\"\n",
    ")\n",
    "\n",
    "# Combine these schemas into a list\n",
    "response_schemas = [title_schema, author_schema, genre_schema, rating_schema]\n",
    "\n",
    "# Now, use these schemas to create a StructuredOutputParser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Generate the format instructions\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Print the format instructions to see what they look like\n",
    "print(format_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ea48afbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The Silent Patient', 'author': 'Alex Michaelides', 'genre': 'psychological thriller', 'rating': '4.5 out of 5'}\n"
     ]
    }
   ],
   "source": [
    "# Define a book review\n",
    "book_review = \"\"\"\n",
    "I just finished reading 'The Silent Patient' by Alex Michaelides, and it was an incredible psychological thriller. The way the author builds suspense is masterful, and I couldn't put it down. I'd give it a solid 4.5 out of 5 stars. Definitely worth a read if you're into thrillers!\n",
    "\"\"\"z\n",
    "\n",
    "# Create a template for the prompt\n",
    "template_string = \"\"\"Extract the following information from the book review that is delimited by triple backticks and return it in JSON format:\n",
    "- Book title\n",
    "- Author name\n",
    "- Genre\n",
    "- User rating out of 5\n",
    "\n",
    "Review: ```{text}```\n",
    "Format the output as JSON with the keys: title, author, genre, rating.\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "# Format the message\n",
    "messages = prompt_template.format_messages(text=book_review, format_instructions=format_instructions)\n",
    "\n",
    "# Use the LLM to get a response\n",
    "response = llm.invoke(messages)  # Assuming `response` is a string\n",
    "\n",
    "# Attempt to parse the string response using the output parser\n",
    "try:\n",
    "    output_dict = output_parser.parse(response)\n",
    "    print(output_dict)\n",
    "except OutputParserException as e:\n",
    "    print(f\"Failed to parse response: {e}\")\n",
    "    print(f\"Raw Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc85522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Class1\n",
    "\n",
    "class FinancialSummary(BaseModel):\n",
    "    company_name: str\n",
    "    total_revenue: float\n",
    "    total_expenses: float\n",
    "    profit: float\n",
    "\n",
    "    def generate(self) -> str:\n",
    "        return f\"\"\"\n",
    "        FINANCIAL SUMMARY:\n",
    "        Company: {self.company_name}\n",
    "        Total Revenue: ${self.total_revenue:,.2f}\n",
    "        Total Expenses: ${self.total_expenses:,.2f}\n",
    "        Net Profit: ${self.profit:,.2f}\n",
    "        \"\"\"\n",
    "\n",
    "# Class 2\n",
    "\n",
    "class ProfitAnalysis(BaseModel):\n",
    "    company_name: str\n",
    "    profit: float\n",
    "\n",
    "    def generate(self) -> str:\n",
    "        return f\"\"\"\n",
    "        PROFIT ANALYSIS:\n",
    "        Company: {self.company_name}\n",
    "        Profit: ${self.profit:,.2f}\n",
    "        \"\"\"\n",
    "# Class 3\n",
    "\n",
    "class RiskAssessment(BaseModel):\n",
    "    company_name: str\n",
    "    risk_factors: List[str] = Field(default_factory=list)\n",
    "\n",
    "    def generate(self) -> str:\n",
    "        risk_factors_formatted = \", \".join(self.risk_factors) if self.risk_factors else \"None\"\n",
    "        return f\"\"\"\n",
    "        RISK ASSESSMENT:\n",
    "        Company: {self.company_name}\n",
    "        Risk Factors: {risk_factors_formatted}\n",
    "        \"\"\"\n",
    "    \n",
    "# Class4 \n",
    "\n",
    "class FinancialReport(BaseModel):\n",
    "    summary: FinancialSummary\n",
    "    profit_analysis: ProfitAnalysis\n",
    "    risk_assessment: RiskAssessment\n",
    "\n",
    "    def generate_full_report(self) -> str:\n",
    "        return \"\\n\".join([\n",
    "            self.summary.generate(),\n",
    "            self.profit_analysis.generate(),\n",
    "            self.risk_assessment.generate()\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "203be968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_user_data() -> FinancialReport:\n",
    "    \n",
    "    company_name=input(\"'Enter Company Name'\")\n",
    "    total_revenue=int(input(\"Enter Revenue\"))\n",
    "    total_expenses=int(input(\"Enter Expense\"))\n",
    "    profit= (total_revenue - total_expenses)\n",
    "\n",
    "        # Sample data\n",
    "    summary_data = FinancialSummary(\n",
    "        company_name=company_name,\n",
    "        total_revenue=total_revenue,\n",
    "        total_expenses=total_expenses,\n",
    "        profit=profit\n",
    "    )\n",
    "\n",
    "    profit_analysis_data = ProfitAnalysis(\n",
    "        company_name=company_name,\n",
    "        profit=profit\n",
    "    )\n",
    "\n",
    "    risk_factors_input = input(\"Enter Risk Factor\")\n",
    "\n",
    "    risk_assessment_data = RiskAssessment(\n",
    "        company_name=company_name,\n",
    "        risk_factors=[factor.strip() for factor in risk_factors_input.split(\",\")] if risk_factors_input else []\n",
    "    )\n",
    "\n",
    "    # Creating the full report\n",
    "    report = FinancialReport(\n",
    "        summary=summary_data,\n",
    "        profit_analysis=profit_analysis_data,\n",
    "        risk_assessment=risk_assessment_data\n",
    "    )\n",
    "    \n",
    "    return report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a099fb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Enter Company Name'Simplilearn\n",
      "Enter Revenue2000000\n",
      "Enter Expense70000\n",
      "Enter Risk FactorMarket Competition, Red Ocean, Changing Landscape\n"
     ]
    }
   ],
   "source": [
    "report = collect_user_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c41ca5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        FINANCIAL SUMMARY:\n",
      "        Company: Simplilearn\n",
      "        Total Revenue: $2,000,000.00\n",
      "        Total Expenses: $70,000.00\n",
      "        Net Profit: $1,930,000.00\n",
      "        \n",
      "\n",
      "        PROFIT ANALYSIS:\n",
      "        Company: Simplilearn\n",
      "        Profit: $1,930,000.00\n",
      "        \n",
      "\n",
      "        RISK ASSESSMENT:\n",
      "        Company: Simplilearn\n",
      "        Risk Factors: Market Competition, Red Ocean, Changing Landscape\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Generate the LLM prompt\n",
    "llm_prompt = report.generate_full_report()\n",
    "print(llm_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2f533b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        STRATEGIC RECOMMENDATIONS:\n",
      "        1. Diversify product offerings to stay ahead of market competition.\n",
      "        2. Invest in research and development to adapt to the changing landscape.\n",
      "        3. Explore new markets and opportunities to mitigate risks associated with a red ocean environment.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(llm_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ff9a127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the best way to learn a new language?\n",
      "Describe the Learning in this one.\n",
      "Summarize our conversation so far in 10 words.\n",
      "The best way to learn a new language is through immersion, practice, and consistency.\n",
      "\n",
      "Learning a new language involves immersion, practice, and consistency.\n",
      "\n",
      "Summary: Immersion, practice, consistency key to learning new language effectively.\n",
      "\n",
      "--- Updated Conversation ---\n",
      "\n",
      "What's the best way to learn a new language?\n",
      "Describe the Learning in this one.\n",
      "Summarize our conversation so far in 10 words.\n",
      "The best way to learn a new language is through immersion, practice, and consistency.\n",
      "\n",
      "Learning a new language involves immersion, practice, and consistency.\n",
      "\n",
      "Summary: Immersion, practice, consistency key to learning new language effectively.\n",
      "Can you give me some specific strategies?\n",
      "Some specific strategies for learning a new language include:\n",
      "\n",
      "1. Immerse yourself in the language by surrounding yourself with native speakers or media in that language.\n",
      "2. Practice speaking, listening, reading, and writing in the language regularly.\n",
      "3. Use language learning apps or programs to supplement your learning.\n",
      "4. Set specific language learning goals and track your progress.\n",
      "5. Join language exchange groups or find a language partner to practice with.\n",
      "6. Incorporate the language into your daily life, such as labeling items in your home with the new language.\n",
      "7. Take language classes or work with a tutor to receive structured guidance and feedback.\n",
      "\n",
      "--- Conversation with Summary ---\n",
      "\n",
      "What's the best way to learn a new language?\n",
      "Describe the Learning in this one.\n",
      "Summarize our conversation so far in 10 words.\n",
      "The best way to learn a new language is through immersion, practice, and consistency.\n",
      "\n",
      "Learning a new language involves immersion, practice, and consistency.\n",
      "\n",
      "Summary: Immersion, practice, consistency key to learning new language effectively.\n",
      "Can you give me some specific strategies?\n",
      "Some specific strategies for learning a new language include:\n",
      "\n",
      "1. Immerse yourself in the language by surrounding yourself with native speakers or media in that language.\n",
      "2. Practice speaking, listening, reading, and writing in the language regularly.\n",
      "3. Use language learning apps or programs to supplement your learning.\n",
      "4. Set specific language learning goals and track your progress.\n",
      "5. Join language exchange groups or find a language partner to practice with.\n",
      "6. Incorporate the language into your daily life, such as labeling items in your home with the new language.\n",
      "7. Take language classes or work with a tutor to receive structured guidance and feedback.\n",
      "Summarize our conversation so far in exactly 10 words.\n",
      "Immersion, practice, consistency key to learning new language effectively.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    # Create the messages list with the user prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Create a chat completion request\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.4,\n",
    "        top_p=0.8\n",
    "        # Degree of randomness in the model's output\n",
    "    )\n",
    "    \n",
    "    # Return the content of the first response\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Step 1: Import the Necessary Components\n",
    "from langchain.prompts import MessagesPlaceholder, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# These are classes which represents the messages froma human user and AI bot\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "# Step 2: Define Message Templates\n",
    "simple_prompt = \"Describe the {subject} in this one.\"\n",
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "\n",
    "# Step 3: Create a Chat Prompt with Placeholders\n",
    "simple_message_template = HumanMessagePromptTemplate.from_template(simple_prompt)\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "# Like the over structure of the conversation between human and AI\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"conversation\"), \n",
    "    # MessagesPlaceholder will allow conversations to be dynamically generated\n",
    "    simple_message_template,\n",
    "    human_message_template\n",
    "])\n",
    "\n",
    "# Step 4: Define the Initial Conversation Messages\n",
    "# Starting the conversation with a user input only, no predefined AI response\n",
    "initial_human_message = HumanMessage(content=\"What's the best way to learn a new language?\")\n",
    "\n",
    "# Step 5: Generate the Initial Conversation\n",
    "initial_conversation = chat_prompt.format_prompt(\n",
    "    conversation=[initial_human_message],\n",
    "    subject=\"Learning\",\n",
    "    word_count=\"10\"\n",
    ").to_messages()\n",
    "\n",
    "#.to_message formats the conversation into a list of messages object (Human and AI)\n",
    "\n",
    "# Step 6: Format the Conversation for the LLM\n",
    "conversation_prompt = \"\\n\".join([message.content for message in initial_conversation])\n",
    "\n",
    "# Step 7: Use the OpenAI API to Generate the First AI Response\n",
    "llm_response = get_completion(conversation_prompt, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Step 8: Add the LLM's Response to the Conversation\n",
    "first_ai_message = AIMessage(content=llm_response)\n",
    "initial_conversation.append(first_ai_message)\n",
    "\n",
    "# Step 9: Print the Conversation\n",
    "for message in initial_conversation:\n",
    "    print(message.content)\n",
    "\n",
    "# Step 10: Continue the Conversation Dynamically\n",
    "# Example of continuing the conversation with a new user input\n",
    "# Assuming the same setup as before\n",
    "\n",
    "# Step 10: Continue the Conversation Dynamically\n",
    "# Example of continuing the conversation with a new user input\n",
    "new_human_message = HumanMessage(content=\"Can you give me some specific strategies?\")\n",
    "initial_conversation.append(new_human_message)\n",
    "\n",
    "# Update the prompt with the latest conversation\n",
    "conversation_prompt = \"\\n\".join([message.content for message in initial_conversation])\n",
    "\n",
    "# Get the next AI response\n",
    "new_llm_response = get_completion(conversation_prompt, model=\"gpt-3.5-turbo\")\n",
    "new_ai_message = AIMessage(content=new_llm_response)\n",
    "initial_conversation.append(new_ai_message)\n",
    "\n",
    "# Print the updated conversation\n",
    "print(\"\\n--- Updated Conversation ---\\n\")\n",
    "for message in initial_conversation:\n",
    "    print(message.content)\n",
    "\n",
    "# Requesting a 10-word summary as a separate, explicit instruction\n",
    "summary_request = \"Summarize our conversation so far in exactly 10 words.\"\n",
    "\n",
    "# Add the summary request to the conversation\n",
    "initial_conversation.append(HumanMessage(content=summary_request))\n",
    "\n",
    "# Update the prompt for summary\n",
    "summary_prompt = \"\\n\".join([message.content for message in initial_conversation])\n",
    "\n",
    "# Get the summary response\n",
    "summary_response = get_completion(summary_prompt, model=\"gpt-3.5-turbo\")\n",
    "summary_ai_message = AIMessage(content=summary_response)\n",
    "initial_conversation.append(summary_ai_message)\n",
    "\n",
    "# Print the conversation including the summary\n",
    "print(\"\\n--- Conversation with Summary ---\\n\")\n",
    "for message in initial_conversation:\n",
    "    print(message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068bc33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca64aa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Smartphone screen goes blank while i am using it to browse internet\n",
      "It seems like you are having trouble with your Smartphone. Letswith you describing the issue\n",
      "Based on your issue described, here are some steps for your Smartphone\n",
      "1. Check if your Smartphone battery is low or drained. If so, charge your Smartphone and try using it again.\n",
      "\n",
      "2. Restart your Smartphone by holding down the power button until the restart option appears. This can help resolve any temporary software glitches causing the screen to go blank.\n",
      "\n",
      "3. Make sure your Smartphone software is up to date. Check for any available updates in the settings menu and install them if necessary.\n",
      "\n",
      "4. If the issue persists, try clearing the cache and data of the browser you are using to browse the internet. This can help resolve any issues related to the browser causing the screen to go blank.\n",
      "\n",
      "5. If none of the above steps work, consider resetting your Smartphone to its factory settings. Make sure to back up any important data before doing so, as this will erase all data on your Smartphone.\n",
      "\n",
      "If the issue still persists after trying these steps, it may be a hardware issue and you may need to contact the manufacturer or a professional technician for further assistance.\n",
      "\n",
      "--- Updated Conversation ---\n",
      "\n",
      "My Smartphone screen goes blank while i am using it to browse internet\n",
      "It seems like you are having trouble with your Smartphone. Letswith you describing the issue\n",
      "Based on your issue described, here are some steps for your Smartphone\n",
      "1. Check if your Smartphone battery is low or drained. If so, charge your Smartphone and try using it again.\n",
      "\n",
      "2. Restart your Smartphone by holding down the power button until the restart option appears. This can help resolve any temporary software glitches causing the screen to go blank.\n",
      "\n",
      "3. Make sure your Smartphone software is up to date. Check for any available updates in the settings menu and install them if necessary.\n",
      "\n",
      "4. If the issue persists, try clearing the cache and data of the browser you are using to browse the internet. This can help resolve any issues related to the browser causing the screen to go blank.\n",
      "\n",
      "5. If none of the above steps work, consider resetting your Smartphone to its factory settings. Make sure to back up any important data before doing so, as this will erase all data on your Smartphone.\n",
      "\n",
      "If the issue still persists after trying these steps, it may be a hardware issue and you may need to contact the manufacturer or a professional technician for further assistance.\n",
      "Can you give me some specific strategies?\n",
      "Sure! Here are some more specific strategies you can try:\n",
      "\n",
      "1. Adjust the screen brightness on your Smartphone. Sometimes the screen may go blank if the brightness is set too low or too high. Try adjusting the brightness to see if it resolves the issue.\n",
      "\n",
      "2. Check for any screen timeout settings on your Smartphone. The screen may go blank if the timeout setting is too short. Increase the timeout duration in the settings menu to see if it helps.\n",
      "\n",
      "3. Try using a different browser to see if the issue is specific to the browser you are currently using. If the screen still goes blank while using a different browser, it may indicate a broader issue with your Smartphone.\n",
      "\n",
      "4. Check for any third-party apps that may be causing conflicts with the screen display. Try uninstalling recently downloaded apps one by one to see if any particular app is causing the issue.\n",
      "\n",
      "5. If you have recently dropped your Smartphone or exposed it to water or extreme temperatures, it may have caused damage to the screen or internal components. In this case, you may need to have your Smartphone repaired or replaced.\n",
      "\n",
      "6. If the issue only occurs while browsing the internet, try clearing the cache and cookies of the browser you are using. This can help resolve any temporary data issues that may be causing the screen to go blank.\n",
      "\n",
      "7. Consider performing a soft reset on your Smartphone by removing the battery (if possible) and reinserting it after a few seconds. This can help reset the device and potentially resolve any software glitches causing the screen to go blank.\n",
      "\n",
      "If none of these strategies work, it may be best to seek professional help or contact the manufacturer for further assistance.\n",
      "\n",
      "--- Conversation with Summary ---\n",
      "\n",
      "My Smartphone screen goes blank while i am using it to browse internet\n",
      "It seems like you are having trouble with your Smartphone. Letswith you describing the issue\n",
      "Based on your issue described, here are some steps for your Smartphone\n",
      "1. Check if your Smartphone battery is low or drained. If so, charge your Smartphone and try using it again.\n",
      "\n",
      "2. Restart your Smartphone by holding down the power button until the restart option appears. This can help resolve any temporary software glitches causing the screen to go blank.\n",
      "\n",
      "3. Make sure your Smartphone software is up to date. Check for any available updates in the settings menu and install them if necessary.\n",
      "\n",
      "4. If the issue persists, try clearing the cache and data of the browser you are using to browse the internet. This can help resolve any issues related to the browser causing the screen to go blank.\n",
      "\n",
      "5. If none of the above steps work, consider resetting your Smartphone to its factory settings. Make sure to back up any important data before doing so, as this will erase all data on your Smartphone.\n",
      "\n",
      "If the issue still persists after trying these steps, it may be a hardware issue and you may need to contact the manufacturer or a professional technician for further assistance.\n",
      "Can you give me some specific strategies?\n",
      "Sure! Here are some more specific strategies you can try:\n",
      "\n",
      "1. Adjust the screen brightness on your Smartphone. Sometimes the screen may go blank if the brightness is set too low or too high. Try adjusting the brightness to see if it resolves the issue.\n",
      "\n",
      "2. Check for any screen timeout settings on your Smartphone. The screen may go blank if the timeout setting is too short. Increase the timeout duration in the settings menu to see if it helps.\n",
      "\n",
      "3. Try using a different browser to see if the issue is specific to the browser you are currently using. If the screen still goes blank while using a different browser, it may indicate a broader issue with your Smartphone.\n",
      "\n",
      "4. Check for any third-party apps that may be causing conflicts with the screen display. Try uninstalling recently downloaded apps one by one to see if any particular app is causing the issue.\n",
      "\n",
      "5. If you have recently dropped your Smartphone or exposed it to water or extreme temperatures, it may have caused damage to the screen or internal components. In this case, you may need to have your Smartphone repaired or replaced.\n",
      "\n",
      "6. If the issue only occurs while browsing the internet, try clearing the cache and cookies of the browser you are using. This can help resolve any temporary data issues that may be causing the screen to go blank.\n",
      "\n",
      "7. Consider performing a soft reset on your Smartphone by removing the battery (if possible) and reinserting it after a few seconds. This can help reset the device and potentially resolve any software glitches causing the screen to go blank.\n",
      "\n",
      "If none of these strategies work, it may be best to seek professional help or contact the manufacturer for further assistance.\n",
      "Summarize our conversation so far 3 bullet points in Capital Case.\n",
      "- TROUBLESHOOTING STEPS FOR SMARTPHONE SCREEN GOING BLANK INCLUDE CHECKING BATTERY, RESTARTING DEVICE, UPDATING SOFTWARE, CLEARING BROWSER CACHE, AND FACTORY RESET\n",
      "- SPECIFIC STRATEGIES INCLUDE ADJUSTING SCREEN BRIGHTNESS, CHANGING SCREEN TIMEOUT SETTINGS, TRYING DIFFERENT BROWSER, UNINSTALLING CONFLICTING APPS, AND CHECKING FOR PHYSICAL DAMAGE\n",
      "- IF ISSUE PERSISTS, SEEK PROFESSIONAL HELP OR CONTACT MANUFACTURER FOR FURTHER ASSISTANCE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    # Create the messages list with the user prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Create a chat completion request\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.4,\n",
    "        top_p=0.8\n",
    "        # Degree of randomness in the model's output\n",
    "    )\n",
    "    \n",
    "    # Return the content of the first response\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Step 1: Import the Necessary Components\n",
    "from langchain.prompts import MessagesPlaceholder, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# These are classes which represents the messages froma human user and AI bot\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "# Step 2: Define Message Templates\n",
    "Issue_prompt = \"It seems like you are having trouble with your {device}. Letswith you describing the issue\"\n",
    "troubleshoot_prompt = \"Based on your issue described, here are some steps for your {device}\"\n",
    "\n",
    "# Step 3: Create a Chat Prompt with Placeholders\n",
    "simple_message_template = HumanMessagePromptTemplate.from_template(Issue_prompt)\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(troubleshoot_prompt)\n",
    "\n",
    "# Like the over structure of the conversation between human and AI\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"conversation\"), \n",
    "    # MessagesPlaceholder will allow conversations to be dynamically generated\n",
    "    simple_message_template,\n",
    "    human_message_template\n",
    "])\n",
    "\n",
    "# Step 4: Define the Initial Conversation Messages\n",
    "# Starting the conversation with a user input only, no predefined AI response\n",
    "initial_human_message = HumanMessage(content= \"My Smartphone screen goes blank while i am using it to browse internet\")\n",
    "\n",
    "# Step 5: Generate the Initial Conversation\n",
    "initial_conversation = chat_prompt.format_prompt(\n",
    "    conversation=[initial_human_message],\n",
    "    device=\"Smartphone\"\n",
    "    \n",
    ").to_messages()\n",
    "\n",
    "#.to_message formats the conversation into a list of messages object (Human and AI)\n",
    "\n",
    "# Step 6: Format the Conversation for the LLM\n",
    "conversation_prompt = \"\\n\".join([message.content for message in initial_conversation])\n",
    "\n",
    "# Step 7: Use the OpenAI API to Generate the First AI Response\n",
    "llm_response = get_completion(conversation_prompt, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Step 8: Add the LLM's Response to the Conversation\n",
    "first_ai_message = AIMessage(content=llm_response)\n",
    "initial_conversation.append(first_ai_message)\n",
    "\n",
    "# Step 9: Print the Conversation\n",
    "for message in initial_conversation:\n",
    "    print(message.content)\n",
    "\n",
    "# Step 10: Continue the Conversation Dynamically\n",
    "# Example of continuing the conversation with a new user input\n",
    "# Assuming the same setup as before\n",
    "\n",
    "# Step 10: Continue the Conversation Dynamically\n",
    "# Example of continuing the conversation with a new user input\n",
    "new_human_message = HumanMessage(content=\"Can you give me some specific strategies?\")\n",
    "initial_conversation.append(new_human_message)\n",
    "\n",
    "# Update the prompt with the latest conversation\n",
    "conversation_prompt = \"\\n\".join([message.content for message in initial_conversation])\n",
    "\n",
    "# Get the next AI response\n",
    "new_llm_response = get_completion(conversation_prompt, model=\"gpt-3.5-turbo\")\n",
    "new_ai_message = AIMessage(content=new_llm_response)\n",
    "initial_conversation.append(new_ai_message)\n",
    "\n",
    "# Print the updated conversation\n",
    "print(\"\\n--- Updated Conversation ---\\n\")\n",
    "for message in initial_conversation:\n",
    "    print(message.content)\n",
    "\n",
    "# Requesting a 10-word summary as a separate, explicit instruction\n",
    "summary_request = \"Summarize our conversation so far 3 bullet points in Capital Case.\"\n",
    "\n",
    "# Add the summary request to the conversation\n",
    "initial_conversation.append(HumanMessage(content=summary_request))\n",
    "\n",
    "# Update the prompt for summary\n",
    "summary_prompt = \"\\n\".join([message.content for message in initial_conversation])\n",
    "\n",
    "# Get the summary response\n",
    "summary_response = get_completion(summary_prompt, model=\"gpt-3.5-turbo\")\n",
    "summary_ai_message = AIMessage(content=summary_response)\n",
    "initial_conversation.append(summary_ai_message)\n",
    "\n",
    "# Print the conversation including the summary\n",
    "print(\"\\n--- Conversation with Summary ---\\n\")\n",
    "for message in initial_conversation:\n",
    "    print(message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ebbcd6",
   "metadata": {},
   "source": [
    "# Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f623222",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Extract the following information from the book review that is delimited by \n",
    "triple backticks:\n",
    "\n",
    "- Book title\n",
    "- Author name\n",
    "- Genre\n",
    "- User rating out of 5. Just the float value\n",
    "\n",
    "Review: ```{text}```\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "book_review = \"\"\"\n",
    "I just finished reading 'The Silent Patient' by Alex Michaelides, \n",
    "and it was an incredible psychological thriller. \n",
    "The way the author builds suspense is masterful, \n",
    "and I couldn't put it down. I'd give it a solid 4.5 out of 5 stars. \n",
    "Definitely worth a read if you're into thrillers!\n",
    "\"\"\"\n",
    "\n",
    "messages = prompt_template.format_messages(text=book_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b926b8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Extract the following information from the book review that is delimited by \\ntriple backticks:\\n\\n- Book title\\n- Author name\\n- Genre\\n- User rating out of 5. Just the float value\\n\\nReview: ```\\nI just finished reading 'The Silent Patient' by Alex Michaelides, \\nand it was an incredible psychological thriller. \\nThe way the author builds suspense is masterful, \\nand I couldn't put it down. I'd give it a solid 4.5 out of 5 stars. \\nDefinitely worth a read if you're into thrillers!\\n```\\n\")]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "706805d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Book title: The Silent Patient\n",
      "Author name: Alex Michaelides\n",
      "Genre: Psychological thriller\n",
      "User rating out of 5: 4.5\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our response is Str but we wanted a Key: Value pair (dict format/JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d24e2f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_parse_response(response):\n",
    "    # Check if response content is available\n",
    "    if not response or not response:\n",
    "        return {\"error\": \"No content in the response\"}\n",
    "\n",
    "    lines = response.splitlines()\n",
    "    parsed_data = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if \"Book title\" in line:\n",
    "            parsed_data[\"title\"] = line.split(\":\")[1].strip()\n",
    "        elif \"Author name\" in line:\n",
    "            parsed_data[\"author\"] = line.split(\":\")[1].strip()\n",
    "        elif \"Genre\" in line:\n",
    "            parsed_data[\"genre\"] = line.split(\":\")[1].strip()\n",
    "        elif \"User rating out of 5\" in line:\n",
    "            parsed_data[\"rating\"] = float(line.split(\":\")[1].strip())\n",
    "    \n",
    "    return parsed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2eaa0246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The Silent Patient', 'author': 'Alex Michaelides', 'genre': 'Psychological thriller', 'rating': 4.5}\n"
     ]
    }
   ],
   "source": [
    "# Now manually parse the response\n",
    "output_dict = manual_parse_response(response)\n",
    "\n",
    "if \"error\" in output_dict:\n",
    "    print(output_dict[\"error\"])\n",
    "else:\n",
    "    print(output_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7955556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Output Parser from Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c1bc2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47e23faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"title\": string  // The title of the book.\n",
      "\t\"author\": string  // The author of the book.\n",
      "\t\"genre\": string  // The genre of the book.\n",
      "\t\"rating\": string  // The user's rating of the book out of 5.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# Define the schema for each expected piece of data\n",
    "title_schema = ResponseSchema(\n",
    "    name=\"title\",\n",
    "    description=\"The title of the book.\"\n",
    ")\n",
    "\n",
    "author_schema = ResponseSchema(\n",
    "    name=\"author\",\n",
    "    description=\"The author of the book.\"\n",
    ")\n",
    "\n",
    "genre_schema = ResponseSchema(\n",
    "    name=\"genre\",\n",
    "    description=\"The genre of the book.\"\n",
    ")\n",
    "\n",
    "rating_schema = ResponseSchema(\n",
    "    name=\"rating\",\n",
    "    description=\"The user's rating of the book out of 5.\"\n",
    ")\n",
    "\n",
    "# Combine these schemas into a list\n",
    "response_schemas = [title_schema, author_schema, genre_schema, rating_schema]\n",
    "\n",
    "# Now, use these schemas to create a StructuredOutputParser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Generate the format instructions\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Print the format instructions to see what they look like\n",
    "print(format_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cdfa39bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The Silent Patient', 'author': 'Alex Michaelides', 'genre': 'Psychological thriller', 'rating': '4.5 out of 5'}\n"
     ]
    }
   ],
   "source": [
    "# Define a book review\n",
    "book_review = \"\"\"\n",
    "I just finished reading 'The Silent Patient' by Alex Michaelides, and it was an incredible psychological thriller. The way the author builds suspense is masterful, and I couldn't put it down. I'd give it a solid 4.5 out of 5 stars. Definitely worth a read if you're into thrillers!\n",
    "\"\"\"\n",
    "\n",
    "# Create a template for the prompt\n",
    "template_string = \"\"\"Extract the following information from the book review that is delimited by triple backticks and return it in JSON format:\n",
    "- Book title\n",
    "- Author name\n",
    "- Genre\n",
    "- User rating out of 5\n",
    "\n",
    "Review: ```{text}```\n",
    "Format the output as JSON with the keys: title, author, genre, rating.\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "# Format the message\n",
    "messages = prompt_template.format_messages(text=book_review, format_instructions=format_instructions)\n",
    "\n",
    "# Use the LLM to get a response\n",
    "response = llm.invoke(messages)  # Assuming `response` is a string\n",
    "\n",
    "# Attempt to parse the string response using the output parser\n",
    "try:\n",
    "    output_dict = output_parser.parse(response)\n",
    "    print(output_dict)\n",
    "except OutputParserException as e:\n",
    "    print(f\"Failed to parse response: {e}\")\n",
    "    print(f\"Raw Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c7151a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# Define the schema for each expected piece of data\n",
    "Product_schema = ResponseSchema(\n",
    "    name=\"Product Name\",\n",
    "    description=\"Name of the product\"\n",
    ")\n",
    "\n",
    "Price_schema = ResponseSchema(\n",
    "    name=\"Price\",\n",
    "    description=\"Price of the product.\"\n",
    ")\n",
    "\n",
    "rating_schema = ResponseSchema(\n",
    "    name=\"rating\",\n",
    "    description=\"Rating of the product.\"\n",
    ")\n",
    "\n",
    "\n",
    "# Combine these schemas into a list\n",
    "response_schemas = [Product_schema, Price_schema, rating_schema]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "15f48464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a template for the prompt\n",
    "template_string = \"\"\"Extract the following information from the product that is delimited by triple backticks and return it in below format:\n",
    "- Product Name\n",
    "- Price\n",
    "- Rating\n",
    "\n",
    "Review: ```{text}```\n",
    "Format the output as follow\n",
    "\n",
    "Product Name      |  Price  |  Rating\n",
    "\n",
    "------------------|---------|---------\n",
    "\n",
    "[Product Name]      [Price]    [Rating]\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "80b4d318",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "\n",
    "# Define a book review\n",
    "product_review = \"\"\"\n",
    "i purchase an apple i phone for 999 dollars last month. \n",
    "It is a great product with so many features and i really liked it alot\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Format the message\n",
    "messages = prompt_template.format_messages(text=product_review, format_instructions=format_instructions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6dfaf8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6afb0c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. The camera quality is amazing and the battery life is also very good. It is very user-friendly and the interface is very smooth. The screen resolution is also top-notch and the design of the phone is sleek and modern. I am very satisfied with my purchase and would highly recommend it to others.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a96cf7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"Product Name\": string  // Name of the product\n",
      "\t\"Price\": string  // Price of the product.\n",
      "\t\"rating\": string  // Rating of the product.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Now, use these schemas to create a StructuredOutputParser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Generate the format instructions\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Print the format instructions to see what they look like\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6bc8a806",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OutputParserException' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/utils/json.py:183\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/utils/json.py:147\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    146\u001b[0m         json_str \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/utils/json.py:163\u001b[0m, in \u001b[0;36m_parse_json\u001b[0;34m(json_str, parser)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/utils/json.py:118\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output_dict)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain/output_parsers/structured.py:97\u001b[0m, in \u001b[0;36mStructuredOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     96\u001b[0m expected_keys \u001b[38;5;241m=\u001b[39m [rs\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_schemas]\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/utils/json.py:185\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid JSON object. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m output_parser\u001b[38;5;241m.\u001b[39mparse(response)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output_dict)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mOutputParserException\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OutputParserException' is not defined"
     ]
    }
   ],
   "source": [
    "# Attempt to parse the string response using the output parser\n",
    "try:\n",
    "    output_dict = output_parser.parse(response)\n",
    "    print(output_dict)\n",
    "except OutputParserException as e:\n",
    "    print(f\"Failed to parse response: {e}\")\n",
    "    print(f\"Raw Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fe66be7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.2.0)\n",
      "Collecting Loader\n",
      "  Downloading loader-2017.9.11-py3-none-any.whl.metadata (623 bytes)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from PyPDF) (4.12.2)\n",
      "Collecting bs4 (from Loader)\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bs4->Loader) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from beautifulsoup4->bs4->Loader) (2.3.2.post1)\n",
      "Downloading loader-2017.9.11-py3-none-any.whl (5.9 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4, Loader\n",
      "Successfully installed Loader-2017.9.11 bs4-0.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df6ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader, PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b827b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"Sample_Langchain_document.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58e8a48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24d39ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Sample_Langchain_document.pdf', 'page': 0}, page_content=\"Zero-shot prompting, despite its revolutionary potential, came with a set of challenges that needed to be addressed to maximize its e<ectiveness. Overcoming these challenges required advances in model architecture, training strategies, and the development of new techniques like prompt engineering. Let's explore the key challenges associated with zero-shot prompting and how they were overcome. 1. Understanding Task Instructions Challenge: One of the primary challenges of zero-shot prompting was ensuring that the AI model could accurately interpret the task being asked of it from the prompt alone. Since the model wasn't explicitly trained on the task, it often struggled to understand what the user wanted, leading to incorrect or irrelevant outputs. Overcoming the Challenge: • Prompt Engineering: Researchers and practitioners developed techniques for crafting prompts that clearly conveyed the task to the model. This involved experimenting with di<erent phrasings, providing explicit instructions, and even using examples within the prompt to guide the model more e<ectively. • Instruction-Based Learning: Models like GPT-3 were trained on datasets that included a wide variety of instructions and their corresponding outputs, helping the model learn to generalize from the instructions provided in the prompt. 2. Sensitivity to Prompt Wording Challenge: Zero-shot models were often highly sensitive to the exact wording of the prompt. Slight changes in phrasing could lead to signiﬁcantly di<erent responses, making the behavior of the model unpredictable and challenging to control. Overcoming the Challenge: • Training on Diverse Data: To reduce sensitivity, models were trained on more diverse datasets that included a variety of ways to express similar tasks or requests. This helped the models become more robust to di<erent phrasings. • Ensembling Prompts: Another approach was to use multiple prompts for the same task and aggregate the results, either by averaging or selecting the most consistent output. This method helped mitigate the e<ect of prompt sensitivity by balancing the model's responses. • Meta-Prompting: Some advanced techniques involved using one AI model to generate prompts for another, creating a layered approach that reduced sensitivity by reﬁning the input before it was processed by the main model. 3. Inconsistency in Output Quality\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8bd07b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Sample_Langchain_document.pdf', 'page': 1}, page_content=\"Challenge: The quality of the output in zero-shot prompting could be inconsistent, especially for complex or domain-speciﬁc tasks. The model might generate accurate responses in one instance and incorrect or nonsensical ones in another, even with similar prompts. Overcoming the Challenge: • Few-Shot Learning: While zero-shot prompting involves no examples, few-shot prompting, where a few examples are included in the prompt, was developed as a way to stabilize and improve output quality. By giving the model a few task-speciﬁc examples within the prompt, the consistency of the output could be signiﬁcantly enhanced. • Reinforcement Learning from Human Feedback (RLHF): Models were ﬁne-tuned using human feedback on generated outputs. By iteratively improving the model based on this feedback, the overall quality and consistency of the model's responses could be enhanced. • Model Calibration: Techniques were developed to calibrate the model's conﬁdence in its predictions, allowing it to indicate when it was more or less certain about a response. This helped users assess the reliability of the outputs. 4. Lack of Domain-Speciﬁc Knowledge Challenge: While zero-shot models performed well on general knowledge tasks, they often lacked the speciﬁc domain expertise needed for specialized tasks (e.g., medical diagnosis, legal advice) because they had not been ﬁne-tuned on domain-speciﬁc data. Overcoming the Challenge: • Domain-Speciﬁc Fine-Tuning: While still maintaining the zero-shot approach, models were sometimes ﬁne-tuned on domain-speciﬁc data without explicitly training on task-speciﬁc examples. This allowed the models to acquire more relevant knowledge while retaining the ﬂexibility of zero-shot prompting. • Hybrid Approaches: Combining zero-shot models with other tools, like knowledge bases or rule-based systems, allowed the AI to access domain-speciﬁc information when needed. This hybrid approach bridged the gap between general and specialized knowledge. • Task-Adaptive Pretraining: Researchers explored task-adaptive pretraining, where models were further pretrained on data related to a speciﬁc domain (like scientiﬁc literature) before being applied to zero-shot tasks in that domain. This enhanced the model's ability to handle specialized content. 5. Managing Biases in Output\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e1d08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Sample_Langchain_document.pdf', 'page': 2}, page_content=\"Challenge: Zero-shot models, like all large language models, were prone to generating biased or inappropriate responses due to the biases present in the training data. This was particularly problematic in zero-shot scenarios where there was no task-speciﬁc training to mitigate these biases. Overcoming the Challenge: • Bias Mitigation Techniques: Various techniques were developed to reduce biases in model outputs, including adversarial training, bias detection algorithms, and post-processing ﬁlters. These approaches helped reduce the likelihood of biased responses in zero-shot scenarios. • Ethical AI Practices: Researchers and developers emphasized the importance of ethical AI practices, including thorough testing and monitoring of AI systems, transparency in how models are trained, and the development of guidelines for responsible AI use. • Diverse Training Data: E<orts were made to train models on more diverse and representative datasets, reducing the extent of inherent biases and improving the fairness of the model's outputs. 6. Limitations in Complex Reasoning Challenge: Zero-shot models initially struggled with tasks requiring complex reasoning, logical deduction, or multi-step problem-solving. The lack of explicit training on such tasks made it di<icult for the models to perform them accurately. Overcoming the Challenge: • Chain-of-Thought Prompting: One technique that emerged was chain-of-thought prompting, where the model is guided through a step-by-step reasoning process within the prompt. This helps the model break down complex problems into manageable parts and improves the accuracy of its reasoning. • Enhanced Model Architectures: Advances in model architectures, such as the introduction of transformer models with deeper layers and more attention heads, allowed models to handle more complex reasoning tasks. • Research into Cognitive AI: Ongoing research in cognitive AI, inspired by human reasoning processes, aims to further enhance the reasoning capabilities of language models, making them more adept at handling complex tasks in zero-shot scenarios. Summary of Challenges and Solutions: 1. Understanding Task Instructions: o Challenge: Di<iculty interpreting tasks from prompts alone.\")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5482995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.11.4)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdfplumber) (10.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9cf663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/v0.2/docs/integrations/document_loaders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65d0b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "\n",
    "pdf_loader = PDFPlumberLoader(\"Sample_Langchain_document.pdf\")\n",
    "documents = pdf_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea476ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Sample_Langchain_document.pdf', 'file_path': 'Sample_Langchain_document.pdf', 'page': 0, 'total_pages': 5, 'Title': 'Sample_Langchain_document', 'Producer': 'macOS Version 14.5 (Build 23F79) Quartz PDFContext', 'Author': 'Raghav Goel', 'Creator': 'Word', 'CreationDate': \"D:20240825134820Z00'00'\", 'ModDate': \"D:20240825134820Z00'00'\"}, page_content=\"Zero-shot prompting, despite its revolutionary potential, came with a set of challenges\\nthat needed to be addressed to maximize its e<ectiveness. Overcoming these\\nchallenges required advances in model architecture, training strategies, and the\\ndevelopment of new techniques like prompt engineering. Let's explore the key\\nchallenges associated with zero-shot prompting and how they were overcome.\\n1. Understanding Task Instructions\\nChallenge: One of the primary challenges of zero-shot prompting was ensuring that the\\nAI model could accurately interpret the task being asked of it from the prompt alone.\\nSince the model wasn't explicitly trained on the task, it often struggled to understand\\nwhat the user wanted, leading to incorrect or irrelevant outputs.\\nOvercoming the Challenge:\\n• Prompt Engineering: Researchers and practitioners developed techniques for\\ncrafting prompts that clearly conveyed the task to the model. This involved\\nexperimenting with di<erent phrasings, providing explicit instructions, and even\\nusing examples within the prompt to guide the model more e<ectively.\\n• Instruction-Based Learning: Models like GPT-3 were trained on datasets that\\nincluded a wide variety of instructions and their corresponding outputs, helping\\nthe model learn to generalize from the instructions provided in the prompt.\\n2. Sensitivity to Prompt Wording\\nChallenge: Zero-shot models were often highly sensitive to the exact wording of the\\nprompt. Slight changes in phrasing could lead to significantly di<erent responses,\\nmaking the behavior of the model unpredictable and challenging to control.\\nOvercoming the Challenge:\\n• Training on Diverse Data: To reduce sensitivity, models were trained on more\\ndiverse datasets that included a variety of ways to express similar tasks or\\nrequests. This helped the models become more robust to di<erent phrasings.\\n• Ensembling Prompts: Another approach was to use multiple prompts for the\\nsame task and aggregate the results, either by averaging or selecting the most\\nconsistent output. This method helped mitigate the e<ect of prompt sensitivity\\nby balancing the model's responses.\\n• Meta-Prompting: Some advanced techniques involved using one AI model to\\ngenerate prompts for another, creating a layered approach that reduced\\nsensitivity by refining the input before it was processed by the main model.\\n3. Inconsistency in Output Quality\\n\"),\n",
       " Document(metadata={'source': 'Sample_Langchain_document.pdf', 'file_path': 'Sample_Langchain_document.pdf', 'page': 1, 'total_pages': 5, 'Title': 'Sample_Langchain_document', 'Producer': 'macOS Version 14.5 (Build 23F79) Quartz PDFContext', 'Author': 'Raghav Goel', 'Creator': 'Word', 'CreationDate': \"D:20240825134820Z00'00'\", 'ModDate': \"D:20240825134820Z00'00'\"}, page_content=\"Challenge: The quality of the output in zero-shot prompting could be inconsistent,\\nespecially for complex or domain-specific tasks. The model might generate accurate\\nresponses in one instance and incorrect or nonsensical ones in another, even with\\nsimilar prompts.\\nOvercoming the Challenge:\\n• Few-Shot Learning: While zero-shot prompting involves no examples, few-shot\\nprompting, where a few examples are included in the prompt, was developed as\\na way to stabilize and improve output quality. By giving the model a few task-\\nspecific examples within the prompt, the consistency of the output could be\\nsignificantly enhanced.\\n• Reinforcement Learning from Human Feedback (RLHF): Models were fine-\\ntuned using human feedback on generated outputs. By iteratively improving the\\nmodel based on this feedback, the overall quality and consistency of the model's\\nresponses could be enhanced.\\n• Model Calibration: Techniques were developed to calibrate the model's\\nconfidence in its predictions, allowing it to indicate when it was more or less\\ncertain about a response. This helped users assess the reliability of the outputs.\\n4. Lack of Domain-Specific Knowledge\\nChallenge: While zero-shot models performed well on general knowledge tasks, they\\noften lacked the specific domain expertise needed for specialized tasks (e.g., medical\\ndiagnosis, legal advice) because they had not been fine-tuned on domain-specific data.\\nOvercoming the Challenge:\\n• Domain-Specific Fine-Tuning: While still maintaining the zero-shot approach,\\nmodels were sometimes fine-tuned on domain-specific data without explicitly\\ntraining on task-specific examples. This allowed the models to acquire more\\nrelevant knowledge while retaining the flexibility of zero-shot prompting.\\n• Hybrid Approaches: Combining zero-shot models with other tools, like\\nknowledge bases or rule-based systems, allowed the AI to access domain-\\nspecific information when needed. This hybrid approach bridged the gap\\nbetween general and specialized knowledge.\\n• Task-Adaptive Pretraining: Researchers explored task-adaptive pretraining,\\nwhere models were further pretrained on data related to a specific domain (like\\nscientific literature) before being applied to zero-shot tasks in that domain. This\\nenhanced the model's ability to handle specialized content.\\n5. Managing Biases in Output\\n\"),\n",
       " Document(metadata={'source': 'Sample_Langchain_document.pdf', 'file_path': 'Sample_Langchain_document.pdf', 'page': 2, 'total_pages': 5, 'Title': 'Sample_Langchain_document', 'Producer': 'macOS Version 14.5 (Build 23F79) Quartz PDFContext', 'Author': 'Raghav Goel', 'Creator': 'Word', 'CreationDate': \"D:20240825134820Z00'00'\", 'ModDate': \"D:20240825134820Z00'00'\"}, page_content=\"Challenge: Zero-shot models, like all large language models, were prone to generating\\nbiased or inappropriate responses due to the biases present in the training data. This\\nwas particularly problematic in zero-shot scenarios where there was no task-specific\\ntraining to mitigate these biases.\\nOvercoming the Challenge:\\n• Bias Mitigation Techniques: Various techniques were developed to reduce\\nbiases in model outputs, including adversarial training, bias detection\\nalgorithms, and post-processing filters. These approaches helped reduce the\\nlikelihood of biased responses in zero-shot scenarios.\\n• Ethical AI Practices: Researchers and developers emphasized the importance\\nof ethical AI practices, including thorough testing and monitoring of AI systems,\\ntransparency in how models are trained, and the development of guidelines for\\nresponsible AI use.\\n• Diverse Training Data: E<orts were made to train models on more diverse and\\nrepresentative datasets, reducing the extent of inherent biases and improving\\nthe fairness of the model's outputs.\\n6. Limitations in Complex Reasoning\\nChallenge: Zero-shot models initially struggled with tasks requiring complex reasoning,\\nlogical deduction, or multi-step problem-solving. The lack of explicit training on such\\ntasks made it di<icult for the models to perform them accurately.\\nOvercoming the Challenge:\\n• Chain-of-Thought Prompting: One technique that emerged was chain-of-\\nthought prompting, where the model is guided through a step-by-step reasoning\\nprocess within the prompt. This helps the model break down complex problems\\ninto manageable parts and improves the accuracy of its reasoning.\\n• Enhanced Model Architectures: Advances in model architectures, such as the\\nintroduction of transformer models with deeper layers and more attention\\nheads, allowed models to handle more complex reasoning tasks.\\n• Research into Cognitive AI: Ongoing research in cognitive AI, inspired by human\\nreasoning processes, aims to further enhance the reasoning capabilities of\\nlanguage models, making them more adept at handling complex tasks in zero-\\nshot scenarios.\\nSummary of Challenges and Solutions:\\n1. Understanding Task Instructions:\\no Challenge: Di<iculty interpreting tasks from prompts alone.\\n\"),\n",
       " Document(metadata={'source': 'Sample_Langchain_document.pdf', 'file_path': 'Sample_Langchain_document.pdf', 'page': 3, 'total_pages': 5, 'Title': 'Sample_Langchain_document', 'Producer': 'macOS Version 14.5 (Build 23F79) Quartz PDFContext', 'Author': 'Raghav Goel', 'Creator': 'Word', 'CreationDate': \"D:20240825134820Z00'00'\", 'ModDate': \"D:20240825134820Z00'00'\"}, page_content='o Solution: Improved prompt engineering and instruction-based learning.\\n2. Sensitivity to Prompt Wording:\\no Challenge: High sensitivity to small changes in prompt wording.\\no Solution: Training on diverse data, ensembling prompts, and meta-\\nprompting.\\n3. Inconsistency in Output Quality:\\no Challenge: Inconsistent responses, especially in complex tasks.\\no Solution: Few-shot learning, reinforcement learning from human\\nfeedback, and model calibration.\\n4. Lack of Domain-Specific Knowledge:\\no Challenge: Inadequate performance on specialized tasks.\\no Solution: Domain-specific fine-tuning, hybrid approaches, and task-\\nadaptive pretraining.\\n5. Managing Biases in Output:\\no Challenge: Generating biased or inappropriate responses.\\no Solution: Bias mitigation techniques, ethical AI practices, and training on\\ndiverse data.\\n6. Limitations in Complex Reasoning:\\no Challenge: Struggles with tasks requiring complex reasoning.\\no Solution: Chain-of-thought prompting, enhanced model architectures,\\nand cognitive AI research.\\nConclusion:\\nZero-shot prompting was a game-changer, but it came with challenges that needed to\\nbe addressed to fully realize its potential. Through a combination of advanced\\ntechniques, improved model architectures, and thoughtful prompt design, many of\\nthese challenges have been mitigated, allowing zero-shot prompting to become a\\npowerful tool in the AI toolkit.\\nGauge Your Understanding:\\nDoes this overview clarify the challenges of zero-shot prompting and how they were\\novercome? Would you like to explore any specific challenges or solutions in more detail,\\nor is there another aspect of zero-shot prompting you’re curious about? Let me know!\\n'),\n",
       " Document(metadata={'source': 'Sample_Langchain_document.pdf', 'file_path': 'Sample_Langchain_document.pdf', 'page': 4, 'total_pages': 5, 'Title': 'Sample_Langchain_document', 'Producer': 'macOS Version 14.5 (Build 23F79) Quartz PDFContext', 'Author': 'Raghav Goel', 'Creator': 'Word', 'CreationDate': \"D:20240825134820Z00'00'\", 'ModDate': \"D:20240825134820Z00'00'\"}, page_content='\\n')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyPDFLoader: Best for standard, text-heavy PDFs.\n",
    "# PDFPlumberLoader: Useful for more complex or scanned PDFs where layout parsing is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "221d73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [\"PDF_TEST.pdf\",'Sample_Langchain_document.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5249a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57098146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    documents = loader.load()\n",
    "    all_docs.extend(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e770608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'PDF_TEST.pdf', 'page': 0}, page_content='Queen Elizabeth I—Speech to the Troops at Tilbury In the 1500s, Spain was considered the most powerful country in the world. When the English became their rivals on the trade routes to the New World, King Phillip II responded by building the Spanish Armada,  When the armada launched to invade Britain, the English were sorely outnumbered in both troops and ships. But one thing they had that Spain didn’t was Queen Elizabeth I. As the Spanish Armada was sailing up the English Channel in 1588, the English land troops gathered at Tilbury, Essex. The queen made a public appearance to boost morale and delivered . She was famously wearing a white velvet dress and a breastplate, presenting herself not as a queen but as a war leader of mythological proportions. To put it lightly, her speech landed. After a nine-day battle, the Spaniards suNered a humiliating defeat and Britain was secured as a world power. Historians are still unclear whether the victory happened thanks to superior war tactics or just  but there’s no doubt that the English troops’ faith in their queen allowed them to secure victory against incredible odds.  '),\n",
       " Document(metadata={'source': 'Sample_Langchain_document.pdf', 'page': 0}, page_content=\"Zero-shot prompting, despite its revolutionary potential, came with a set of challenges that needed to be addressed to maximize its e<ectiveness. Overcoming these challenges required advances in model architecture, training strategies, and the development of new techniques like prompt engineering. Let's explore the key challenges associated with zero-shot prompting and how they were overcome. 1. Understanding Task Instructions Challenge: One of the primary challenges of zero-shot prompting was ensuring that the AI model could accurately interpret the task being asked of it from the prompt alone. Since the model wasn't explicitly trained on the task, it often struggled to understand what the user wanted, leading to incorrect or irrelevant outputs. Overcoming the Challenge: • Prompt Engineering: Researchers and practitioners developed techniques for crafting prompts that clearly conveyed the task to the model. This involved experimenting with di<erent phrasings, providing explicit instructions, and even using examples within the prompt to guide the model more e<ectively. • Instruction-Based Learning: Models like GPT-3 were trained on datasets that included a wide variety of instructions and their corresponding outputs, helping the model learn to generalize from the instructions provided in the prompt. 2. Sensitivity to Prompt Wording Challenge: Zero-shot models were often highly sensitive to the exact wording of the prompt. Slight changes in phrasing could lead to signiﬁcantly di<erent responses, making the behavior of the model unpredictable and challenging to control. Overcoming the Challenge: • Training on Diverse Data: To reduce sensitivity, models were trained on more diverse datasets that included a variety of ways to express similar tasks or requests. This helped the models become more robust to di<erent phrasings. • Ensembling Prompts: Another approach was to use multiple prompts for the same task and aggregate the results, either by averaging or selecting the most consistent output. This method helped mitigate the e<ect of prompt sensitivity by balancing the model's responses. • Meta-Prompting: Some advanced techniques involved using one AI model to generate prompts for another, creating a layered approach that reduced sensitivity by reﬁning the input before it was processed by the main model. 3. Inconsistency in Output Quality \"),\n",
       " Document(metadata={'source': 'Sample_Langchain_document.pdf', 'page': 1}, page_content=\"Challenge: The quality of the output in zero-shot prompting could be inconsistent, especially for complex or domain-speciﬁc tasks. The model might generate accurate responses in one instance and incorrect or nonsensical ones in another, even with similar prompts. Overcoming the Challenge: • Few-Shot Learning: While zero-shot prompting involves no examples, few-shot prompting, where a few examples are included in the prompt, was developed as a way to stabilize and improve output quality. By giving the model a few task-speciﬁc examples within the prompt, the consistency of the output could be signiﬁcantly enhanced. • Reinforcement Learning from Human Feedback (RLHF): Models were ﬁne-tuned using human feedback on generated outputs. By iteratively improving the model based on this feedback, the overall quality and consistency of the model's responses could be enhanced. • Model Calibration: Techniques were developed to calibrate the model's conﬁdence in its predictions, allowing it to indicate when it was more or less certain about a response. This helped users assess the reliability of the outputs. 4. Lack of Domain-Speciﬁc Knowledge Challenge: While zero-shot models performed well on general knowledge tasks, they often lacked the speciﬁc domain expertise needed for specialized tasks (e.g., medical diagnosis, legal advice) because they had not been ﬁne-tuned on domain-speciﬁc data. Overcoming the Challenge: • Domain-Speciﬁc Fine-Tuning: While still maintaining the zero-shot approach, models were sometimes ﬁne-tuned on domain-speciﬁc data without explicitly training on task-speciﬁc examples. This allowed the models to acquire more relevant knowledge while retaining the ﬂexibility of zero-shot prompting. • Hybrid Approaches: Combining zero-shot models with other tools, like knowledge bases or rule-based systems, allowed the AI to access domain-speciﬁc information when needed. This hybrid approach bridged the gap between general and specialized knowledge. • Task-Adaptive Pretraining: Researchers explored task-adaptive pretraining, where models were further pretrained on data related to a speciﬁc domain (like scientiﬁc literature) before being applied to zero-shot tasks in that domain. This enhanced the model's ability to handle specialized content. 5. Managing Biases in Output \"),\n",
       " Document(metadata={'source': 'Sample_Langchain_document.pdf', 'page': 2}, page_content=\"Challenge: Zero-shot models, like all large language models, were prone to generating biased or inappropriate responses due to the biases present in the training data. This was particularly problematic in zero-shot scenarios where there was no task-speciﬁc training to mitigate these biases. Overcoming the Challenge: • Bias Mitigation Techniques: Various techniques were developed to reduce biases in model outputs, including adversarial training, bias detection algorithms, and post-processing ﬁlters. These approaches helped reduce the likelihood of biased responses in zero-shot scenarios. • Ethical AI Practices: Researchers and developers emphasized the importance of ethical AI practices, including thorough testing and monitoring of AI systems, transparency in how models are trained, and the development of guidelines for responsible AI use. • Diverse Training Data: E<orts were made to train models on more diverse and representative datasets, reducing the extent of inherent biases and improving the fairness of the model's outputs. 6. Limitations in Complex Reasoning Challenge: Zero-shot models initially struggled with tasks requiring complex reasoning, logical deduction, or multi-step problem-solving. The lack of explicit training on such tasks made it di<icult for the models to perform them accurately. Overcoming the Challenge: • Chain-of-Thought Prompting: One technique that emerged was chain-of-thought prompting, where the model is guided through a step-by-step reasoning process within the prompt. This helps the model break down complex problems into manageable parts and improves the accuracy of its reasoning. • Enhanced Model Architectures: Advances in model architectures, such as the introduction of transformer models with deeper layers and more attention heads, allowed models to handle more complex reasoning tasks. • Research into Cognitive AI: Ongoing research in cognitive AI, inspired by human reasoning processes, aims to further enhance the reasoning capabilities of language models, making them more adept at handling complex tasks in zero-shot scenarios. Summary of Challenges and Solutions: 1. Understanding Task Instructions: o Challenge: Di<iculty interpreting tasks from prompts alone. \"),\n",
       " Document(metadata={'source': 'Sample_Langchain_document.pdf', 'page': 3}, page_content='o Solution: Improved prompt engineering and instruction-based learning. 2. Sensitivity to Prompt Wording: o Challenge: High sensitivity to small changes in prompt wording. o Solution: Training on diverse data, ensembling prompts, and meta-prompting. 3. Inconsistency in Output Quality: o Challenge: Inconsistent responses, especially in complex tasks. o Solution: Few-shot learning, reinforcement learning from human feedback, and model calibration. 4. Lack of Domain-Speciﬁc Knowledge: o Challenge: Inadequate performance on specialized tasks. o Solution: Domain-speciﬁc ﬁne-tuning, hybrid approaches, and task-adaptive pretraining. 5. Managing Biases in Output: o Challenge: Generating biased or inappropriate responses. o Solution: Bias mitigation techniques, ethical AI practices, and training on diverse data. 6. Limitations in Complex Reasoning: o Challenge: Struggles with tasks requiring complex reasoning. o Solution: Chain-of-thought prompting, enhanced model architectures, and cognitive AI research. Conclusion: Zero-shot prompting was a game-changer, but it came with challenges that needed to be addressed to fully realize its potential. Through a combination of advanced techniques, improved model architectures, and thoughtful prompt design, many of these challenges have been mitigated, allowing zero-shot prompting to become a powerful tool in the AI toolkit. Gauge Your Understanding: Does this overview clarify the challenges of zero-shot prompting and how they were overcome? Would you like to explore any speciﬁc challenges or solutions in more detail, or is there another aspect of zero-shot prompting you’re curious about? Let me know! '),\n",
       " Document(metadata={'source': 'Sample_Langchain_document.pdf', 'page': 4}, page_content=' ')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f25563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0A473AA0-F9FD-4762-97FF-383796A62823.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d8bae51",
   "metadata": {},
   "outputs": [
    {
     "ename": "PDFSyntaxError",
     "evalue": "No /Root object! - Is this really a PDF?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPDFSyntaxError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from langchain.document_loaders import PDFPlumberLoader\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pdf_loader \u001b[38;5;241m=\u001b[39m PDFPlumberLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0A473AA0-F9FD-4762-97FF-383796A62823.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mpdf_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:580\u001b[0m, in \u001b[0;36mPDFPlumberLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    579\u001b[0m     blob \u001b[38;5;241m=\u001b[39m Blob\u001b[38;5;241m.\u001b[39mfrom_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/document_loaders/base.py:126\u001b[0m, in \u001b[0;36mBaseBlobParser.parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, blob: Blob) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Eagerly parse the blob into a document or documents.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    This is a convenience method for interactive development environment.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m        List of documents\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_community/document_loaders/parsers/pdf.py:357\u001b[0m, in \u001b[0;36mPDFPlumberParser.lazy_parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdfplumber\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mas_bytes_io() \u001b[38;5;28;01mas\u001b[39;00m file_path:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mpdfplumber\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# open document\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m [\n\u001b[1;32m    360\u001b[0m         Document(\n\u001b[1;32m    361\u001b[0m             page_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_page_content(page)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mpages\n\u001b[1;32m    379\u001b[0m     ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pdfplumber/pdf.py:101\u001b[0m, in \u001b[0;36mPDF.open\u001b[0;34m(cls, path_or_fp, pages, laparams, password, strict_metadata, unicode_norm, repair, gs_path, repair_setting)\u001b[0m\n\u001b[1;32m     98\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlaparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlaparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43municode_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_is_external\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_is_external\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PSException:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream_is_external:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pdfplumber/pdf.py:47\u001b[0m, in \u001b[0;36mPDF.__init__\u001b[0;34m(self, stream, stream_is_external, path, pages, laparams, password, strict_metadata, unicode_norm)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpassword \u001b[38;5;241m=\u001b[39m password\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39municode_norm \u001b[38;5;241m=\u001b[39m unicode_norm\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m \u001b[43mPDFDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPDFParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrsrcmgr \u001b[38;5;241m=\u001b[39m PDFResourceManager()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pdfminer/pdfdocument.py:752\u001b[0m, in \u001b[0;36mPDFDocument.__init__\u001b[0;34m(self, parser, password, caching, fallback)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 752\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFSyntaxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo /Root object! - Is this really a PDF?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m LITERAL_CATALOG:\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mSTRICT:\n",
      "\u001b[0;31mPDFSyntaxError\u001b[0m: No /Root object! - Is this really a PDF?"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "\n",
    "pdf_loader = PDFPlumberLoader(\"0A473AA0-F9FD-4762-97FF-383796A62823.pdf\")\n",
    "documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016fe418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "\n",
    "# from langchain.document_loaders import PDFPlumberLoader\n",
    "\n",
    "# pdf_loader = PDFPlumberLoader(\"PDF_TEST.pdf\")\n",
    "# documents = pdf_loader.load()\n",
    "\n",
    "\n",
    "# from langchain.document_loaders import Docx2txtLoader\n",
    "\n",
    "# word_loader = Docx2txtLoader(\"Sample_Langchain_document.docx\")\n",
    "# documents = word_loader.load()\n",
    "\n",
    "\n",
    "# from langchain.document_loaders import TextLoader\n",
    "\n",
    "# text_loader = TextLoader(\"path/to/your/file.txt\")\n",
    "# documents = text_loader.load()\n",
    "\n",
    "\n",
    "# from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# web_loader = WebBaseLoader(\"https://simplilearn.com\")\n",
    "# documents = web_loader.load()\n",
    "\n",
    "\n",
    "# from langchain.document_loaders import BSHTMLLoader\n",
    "\n",
    "# web_loader = BSHTMLLoader(\"https://lms.simplilearn.com/dashboard\")\n",
    "# documents = web_loader.load()\n",
    "\n",
    "\n",
    "# from langchain.document_loaders import CSVLoader\n",
    "\n",
    "# csv_loader = CSVLoader(\"path/to/your/file.csv\")\n",
    "# documents = csv_loader.load()\n",
    "\n",
    "\n",
    "# from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa9365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
